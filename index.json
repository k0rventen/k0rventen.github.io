[{"content":"I always had some form of homelab/servers running at home. For a while I had a k8s cluster composed of 3 Raspberry pi 4(8g), then a more traditionnal x86 pc, then back on some raspberry pis..\nI changed quite a few things this year, having moved from macOS to linux on my work computer, and my own macbook pro 2015 being replaced by an iPad for media consumption, and my linux desktop for gaming/dev etc..\nI\u0026rsquo;ve also decided to dive into selfhosting a bit more. So here is my complete, end of 2024 setup:\na raspberry pi 4. 8Go of RAM, a 128Go SD card. that\u0026rsquo;s it. No need for a complex k8s cluster, with 20 VMs and 5 control planes. The pi is running raspberry pi OS 64b lite. I removed some stuff:\nsudo apt purge dphys-swapfile avahi-daemon modemmanager triggerhappy --auto-remove mainly dphys-swapfile, to avoid swapping to the sd card. I\u0026rsquo;ve had raspberry pis for a decade now, and none of my sd cards have failed, I believe in part due to not swapping on them. Moreover, the 8Go of RAM of this PI is way more than enough to run what I need.\nIt\u0026rsquo;s running a docker compose based stack. Here is an overview:\nBasically, it\u0026rsquo;s my main server that handles:\nDNS \u0026amp; DHCP on my LAN, using adguard DNS for my tailscale network, using a split horizon DNS setup (again with adguard) Homekit integration for some very specific things using homebridge 3D printing server for my ender3v2 using octoprint centralised password management using vaultwarden (and bitwarden\u0026rsquo;s extensions on all my devices) a simple network share with samba A digital vault for my important documents using paperless-ngx photo and video management with immich A Magic-Mirror server Local GitHub and CI/CD server using gitea basic monitoring of services, servers and such using a TIG stack telegraf/influx/grafana a remote code-server instance for easy development on the pi All of the above are reverse proxied with SSL termination using traefik, adn the CA/certs are generated with minica All of this is running, and the pi isn\u0026rsquo;t doing much:\nRegarding the compose stack, it\u0026rsquo;s all in a services/ folder:\n✓ pi4-carbon:~/services \u0026gt; tree -L 1 . ├── adguard ├── adguard-external ├── docker-compose.yml ├── gitea ├── grafana ├── homebridge ├── immich ├── influx ├── magic-mirror ├── octoprint ├── paperless ├── samba ├── telegraf ├── traefik └── vaultwarden Each folder contains the data and configuration for that specific service, for example grafana:\n✓ pi4-carbon:~/services \u0026gt; tree -L 2 grafana/ grafana/ ├── conf │ └── grafana.ini └── data ├── alerting ├── csv ├── file-collections ├── grafana-apiserver ├── grafana.db ├── pdf ├── plugins └── png And here is an excerpt of the docker-compose:\nservices: # Reverse proxy traefik: image: traefik:comte restart: always ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - ./traefik/traefik.yaml:/etc/traefik/traefik.yaml - ./traefik/certs/:/certs/ - /var/run/docker.sock:/var/run/docker.sock # DNS blackhole # network mode because its acting as a DNS and DHCP server on the LAN adguard: image: adguard/adguardhome restart: always network_mode: host volumes: - ./adguard/conf:/opt/adguardhome/conf - ./adguard/data:/opt/adguardhome/work # Split horizon DNS for the tailscale network # this is configured as the DNS server for the tailnet # and redirects the same internal domains to the tailscale IPs # instead of the local ones adguard-external: image: adguard/adguardhome restart: always ports: - 192.168.1.1:11081:11081 - 100.101.101.101:53:53/udp volumes: - ./adguard-external/conf:/opt/adguardhome/conf - ./adguard-external/data:/opt/adguardhome/work ... Around this central server are a few components:\nthe pi is part of my tailscale mesh network, and is configured to be the DNS. This allows my roaming phone/iPad to 1. get access to all my services wherever I am, like passwords, files etc.. and 2. have DNS queries go through my adblock, removing ads also on the go. a usb-attached SSD, for daily encrypted backup using restic The configuration is done through some systemd units: A mnt-backup.mount for the disk:\n[Unit] Description=Restic Backup External Disk mount [Mount] What=/dev/disk/by-label/backup Where=/mnt/backup Type=ext4 Options=defaults [Install] WantedBy=multi-user.target A restic.service that starts the backup\n[Unit] Description=Automated backup After=mnt-backup.mount BindsTo=mnt-backup.mount PropagatesStopTo=mnt-backup.mount [Install] WantedBy=default.target [Service] Type=simple Environment=\u0026#34;HOME=/home/coco\u0026#34; WorkingDirectory=/home/coco ExecStart=restic -p %d/restic -r /mnt/backup/backups backup . SetCredentialEncrypted=restic: \\ Whxqht+dQJax1aZeCGLxm... And a restic.timer that calls the service\n[Unit] Description=Run backup every day at 2 AM [Timer] OnCalendar=02:00 [Install] WantedBy=timers.target I\u0026rsquo;ve also configured a rpi with a hard drive at my parent\u0026rsquo;s, also part of my tailnet. This is for a remote encrypted backup, using restic as well. Same setup, but the units are a bit different, with a restic-offsite.service:\n[Unit] Description=Automated backup Wants=network.target After=network-online.target [Install] WantedBy=default.target [Service] Type=oneshot Environment=\u0026#34;HOME=/home/coco\u0026#34; WorkingDirectory=/home/coco ExecStart=ssh pi@100.64.95.65 sudo systemctl start mnt-backup.mount ExecStart=restic -p %d/restic -r sftp:pi@100.64.95.65:/mnt/backup/backups backup . ExecStart=ssh pi@100.64.95.65 sudo systemctl stop mnt-backup.mount SetCredentialEncrypted=restic: \\ Whxqht+dQJax... And a restic-offsite.timer:\n[Unit] Description=Run external backup every day at 3 AM [Timer] OnCalendar=*-*-* 03:00 [Install] WantedBy=timers.target On the receiving Pi, i have a similar mnt-backup.mount that mounts the usb hdd when needed. The SetCredentialEncrypted value is created using systemd-ask-password -n | systemd-creds encrypt --name=restic -p - -.\nAnd that\u0026rsquo;s pretty much it. Some automation are handled through Gitea Actions, using a schedule:\non: schedule: - cron: \u0026#39;30 10 * * 1-5\u0026#39; jobs: run_on_schedule: runs-on: ubuntu-latest steps: - name: Pull image and run run: | do stuff And a final automated thing is the badging at $work, which is triggered through a iOS Shortcut, that enables tailscale on my phone, uses the \u0026lsquo;Run script over SSH\u0026rsquo; to start a playwright python script on my server to log and clock in.\nFinally, a look at my Grafana dashboard. The monitoring part isn\u0026rsquo;t really what i\u0026rsquo;m interested in, i mostly glance at it to spot something out of the ordinary, but I\u0026rsquo;ve configured some shortcuts for the most used services up top:\nWe\u0026rsquo;ll see what changes in 2025 ! Happy new year !\n","permalink":"/posts/state-of-the-setup-2024/","summary":"\u003cp\u003eI always had some form of homelab/servers running at home. For a while I had a k8s cluster composed of 3 Raspberry pi 4(8g), then a more traditionnal x86 pc, then back on some raspberry pis..\u003c/p\u003e\n\u003cp\u003eI changed quite a few things this year, having moved from macOS to linux on my work computer, and my own macbook pro 2015 being replaced by an iPad for media consumption, and my linux desktop for gaming/dev etc..\u003c/p\u003e","title":"State of the Setup 2024"},{"content":"What and why Since the Winter 2023 Holiday Update, a new feature appeared on my model 3 Highland: you could change the lock sound to some sounds provided by Tesla, or provide your own, by putting a LockChime.wav file at the root of the TESLADRIVE usb key.\nI\u0026rsquo;ve been playing with it with various sounds found here and there, but I still had to open the glovebox, take the USB key out, change the file and plug it back in.\nI then realized that some raspberry pis (the Zeros W and 2 can, as the 4b)can act as a mass storage device through their usb cable. Wouldn\u0026rsquo;t that be cool to change the lock sound of the car every time ?\nHow First, let\u0026rsquo;s flash a Raspberry Pi 4 with a Lite, 64b version of raspberry pi OS.\nThen, we\u0026rsquo;ll do the following:\nremove unused packages and services to speed up boot time enable the g_mass_storage kernel module (so the Tesla sees a mountable USB partition) create our drive image and script to handle mounting/unmounting test and enjoy ! Once booted, ssh/console into it, and apply the following configuration:\nremove some useless stuff: remove packages\nsudo apt purge triggerhappy avahi-daemon cron modemmanager dphys-swapfile --auto-remove This will remove packages and services that we don\u0026rsquo;t need, specifically swap, avahi..\ndisable unused services Then disable some services that can affect our boot time, mainly around BLE:\nsudo systemctl disable --now bluetooth.service hciuart.service keyboard-setup.service systemd-timesyncd.service sudo systemctl mask bluetooth.service hciuart.service keyboard-setup.service systemd-timesyncd.service enable the g_mass_storage kernel module Append to the end of /boot/firmware/cmdline.txt:\n... modules-load=dwc2,g_mass_storage This will load the needed kernel modules for the pi to act as a mass storage device\nAt the end of /boot/firmware/config.txt\n[all] dtoverlay=dwc2 boot_delay=0 (You can also comment some stuff in the upper sections, like camera_auto_detect. should make the boot faster, not measured)\nthe actual reason we are doing this Create a big 32Go+ file using fallocate (Sentry Mode will not work if size \u0026lt; 32G)\nsudo fallocate /tesladrive -l 48G Create the main script in /usr/local/bin/tesladrive.sh\n#!/bin/bash # mount as loop mkdir -p /mnt/tesladrive losetup /dev/loop2 /tesladrive mount -t exfat -o offset=1048576,time_offset=-420 /dev/loop2 /mnt/tesladrive # swap the lock chime cp $(find /mnt/tesladrive/chimes/ -maxdepth 1 -type f | shuf -n 1) /mnt/tesladrive/LockChime.wav # unmount umount /mnt/tesladrive losetup -d /dev/loop2 # g_mass_storage modprobe g_mass_storage file=/tesladrive stall=0 The script is fairly simple, it is left as an exercise for the reader (and future me) to figure it out. The only weird thing is the offset argument to the mount command. This is because the device is /dev/loop2, but the exfat partition is actually /dev/loop2p1, which isn\u0026rsquo;t starting on the same block (thx exfat). So to mount it, you must tell mount how much offset to the actual start of the exfat partition.\nHere is what fdisk returns for our loopdevice:\nDisk /dev/loop2: 48 GiB, 51539607552 bytes, 100663296 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x83f38a18 Device Boot Start End Sectors Size Id Type /dev/loop2p1 2048 100663295 100661248 48G 7 HPFS/NTFS/exFAT So to compute the offset for mount, we do start sector * sector size, in our case 2048*512=1048576\nYour values might be different.\nCreate a service file for it: The goal here is to make is start as early as possible during the boot process, hence the basic target. Put it in /etc/systemd/system/tesladrive.service\n[Unit] Description=Launch the tesla drive module as fast as possible Before=basic.target After=local-fs.target sysinit.target DefaultDependencies=no [Service] ExecStart=/home/coco/tesladrive.sh [Install] WantedBy=basic.target then enable the service:\nsystemctl enable tesladrive create the fs, add your sounds and enjoy ! Now, You can mount manually the partition and format at as exFat:\nlosetup /dev/loop2 /tesladrive mkfs.exfat /dev/loop2 Note: I had an issue where the Tesla did not recognized the partition properly, and I had to format it from the Tesla.\nInside, we\u0026rsquo;ll create a chimes dir that will hold the available lock sounds. The script will pick one randomly and set it as the default one each time.\nHere is how the file structure should look like:\ncoco@teslapi:~ $ ls /mnt/tesladrive/ LockChime.wav TeslaCam chimes coco@teslapi:~ $ ls /mnt/tesladrive/chimes/ 90s-modem.wav bipbip.wav eternity.wav ogs pikachu.wav airplane-seat-belt.wav canttouchthis.wav hadouken.wav pac-man-die.wav You can find locksounds online, for example here: https://www.notateslaapp.com/tesla-custom-lock-sounds/\nOne thing to note is that the volume can be quite high on some of them. I used sox to reduce it for all of them, as to not be overly loud (but still funny):\nsox -v 0.3 lock.wav lock_low.wav (From https://stackoverflow.com/questions/21776073/reduce-volume-of-audio-file-by-percentage-value-using-sox)\nFinally, plug the Pi in the USB port in the Tesla\u0026rsquo;s glovebox, select Lock sound: \u0026lsquo;USB\u0026rsquo;, then step away from the car.\nEvery time the car starts up, this will start the Pi, which will swap the lock sound before mounting the partition as a mass storage for the tesla, changing the lock sound !\n","permalink":"/posts/tesla-lock-sounds/","summary":"\u003ch1 id=\"what-and-why\"\u003eWhat and why\u003c/h1\u003e\n\u003cp\u003eSince the Winter 2023 Holiday Update, a new feature appeared on my model 3 Highland: you could change the lock sound to some sounds provided by Tesla, or provide your own, by putting a \u003ccode\u003eLockChime.wav\u003c/code\u003e file at the root of the TESLADRIVE usb key.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve been playing with it with various sounds found here and there, but I still had to open the glovebox, take the USB key out, change the file and plug it back in.\u003c/p\u003e","title":"Randomized Tesla Lock Sounds using a Pi"},{"content":"Transmit paper-like messages from 18000 kms away:\nA simple, e-ink display that receives messages from loved ones from the other side of the globe.\nwhy About 10 months ago, my sister left our country to do a WHP (Work-Holiday-Program) on the other side of the world (fun fact, this is almost true as the antipodes of France where we live is right next to New-Zealand, where my sister went).\nGoing far today isn\u0026rsquo;t as bad as it would have been decades ago, we have messages, video calling and other new mediums that can bring people closer together even far away. This was important for all of us to still be able to connect, but I guess even more for my parents. So a few months in, I decided to create something different for them to know what she was up to, without needing their phones.\nThe idea came after seeing the Poem 1 concept, which is a \u0026lsquo;clock\u0026rsquo;, powered by ChatGPT and using a e-ink display. I I thought to myself that this form-factor and display could be a nice way of displaying messages from my sister. They wouldn\u0026rsquo;t diseappear, always readable from anywhere in a room.\nSo the goal was set: My sister would be able to send messages to the e-ink box at my parent\u0026rsquo;s, which would display them on the e-ink screen. How hard could it be ?\nhow This project is made of various pieces:\nthe physical \u0026lsquo;box\u0026rsquo; that will be at my parent\u0026rsquo;s, receiving messages from a server and displaying them a server, that will let clients send messages through a simple front end a way for clients to connect to the server from anywhere in the world The box The first order of business was to find an E-ink display that would fit my requirements. I settled on a 4.2\u0026quot; unit from Waveshare: https://www.waveshare.com/4.2inch-e-paper-module.htm\nThe SPI interface would be controlled through a Raspberry Pi Zero W.\nI then designed a 3D printed case that would fit the two components. As I don\u0026rsquo;t know anything about CAD design tools, I fiddled with Tinkercad until I had something that was both printable and not too ugly (I discovered why beveled and rounded edges are everywhere after printing my first \u0026lsquo;cubic\u0026rsquo; version).\nI then \u0026lsquo;assembled\u0026rsquo; everything:\nsoldered the wires from the display PCB to the pi W, fitted the display and pi inside the box (read double sided tape and hot glue) Here is what it looks like inside:\nAfter some sanding of the exterior of the box, it has quite a nice finish to it and looks decent enough:\nThere should have been a video here but your browser does not seem to support it. Now that we have a working physical thing, onto the software !\nthe software There\u0026rsquo;s multiple facets to consider:\nclients that should be able to send messages easily to a server the server that would store the messages (and pictures, because I decided mid-project that sending pictures to the display would be fun) the logic to display nicely the message and when it was sent on the display. the frontend The first part was a nice challenge. I\u0026rsquo;m not a frontend engineer, and have very limited design abilities (as seen previously on the box\u0026rsquo;s design). I decided that the frontend would be entirely developed (ie the code) by chatGPT. I wanted to see if it was possible to create the whole frontend design and logic using only AI. The features of the frontend would be as follows:\nauthenticate using basic auth a simple form to send a new message/image display the list of previouly sent messages Here is the first message of a long conversation between me and chatGPT:\nWe then refined the UI, implemented the various features (when I say we, I mean I expressed what I wanted, or wrote feedback on the previous iteration).\nHere is a quick list of features that chatGPT implemented on its own:\nall the design, including the nice background gradient handling the requests to the backend server displaying the previous messages from a GET endpoint Adding support for uploading images the little character counter next to the input The only feature that it wasn\u0026rsquo;t able to code properly is the button for removing the selected image.\nFinally, here is what the UI looks like on a phone :\nI would say that 95% of the code was written by gpt, and a few tweaks and corrections by me. Obviously it\u0026rsquo;s quite a simple use-case, but it delivered something I would have spent 3 weeks dealing with CSS centering.\n10/10 experience.\nthe server So this frontend talks to a python backend. Nothing fancy. The data is stored using tinyDB, as an array:\n\u0026#34;32\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1711017153, \u0026#34;user\u0026#34;: \u0026#34;mel\u0026#34;, \u0026#34;display\u0026#34;: \u0026#34;8b74c00a-1093-41da-89ad-a17aabcbbf98\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\\u00e7a apprend \\u00e0 surfer par ici \\ud83c\\udfc4\\u200d\\u2640\\ufe0f\u0026#34; }, \u0026#34;33\u0026#34;: { \u0026#34;timestamp\u0026#34;: 1711513109, \u0026#34;user\u0026#34;: \u0026#34;mel\u0026#34;, \u0026#34;display\u0026#34;: \u0026#34;8b74c00a-1093-41da-89ad-a17aabcbbf98\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;jour 190 - je quitte Mirissa et retourne \\u00e0 Weligama! \\ud83c\\udf0a\u0026#34; } Creating a new message is a POST request. We grab the username, timestamp, message/image, create a UUID for the image and stores it locally. The convert/thumbnail is here to format the image to be then displayed on the e-ink screen. You can imagine the GET endpoint for the display, it\u0026rsquo;s simply returning the last entry of the messages.\n@api.post(\u0026#34;/messages\u0026#34;) async def new_message(resp: Response,request:Request,user= Depends(auth_check)): form = await request.form() msg = {\u0026#34;timestamp\u0026#34;:int(time()),\u0026#34;user\u0026#34;:user} if img := form.get(\u0026#34;image\u0026#34;): im = Image.open(BytesIO(img.file.read())) image = im.convert(\u0026#34;L\u0026#34;) image.thumbnail((368,200)) im_uuid = str(uuid4()) image.save(f\u0026#34;./static/imgs/{im_uuid}.jpg\u0026#34;) msg[\u0026#34;image\u0026#34;] = im_uuid msg[\u0026#34;message\u0026#34;] = \u0026#34;image\u0026#34; if message := form.get(\u0026#34;message\u0026#34;): msg[\u0026#34;message\u0026#34;] = message messages.insert(msg) return \u0026#34;ok\u0026#34; the network Ok so we have clients, a server, and a display, all on different part of the world. The clients are roaming (mainly my sister across the globe), the server is on my rasberry pi at home, and the display is at my parent\u0026rsquo;s. I almost went the cloud way, with a hosted, public server, but that would mean spending time to enhance the app\u0026rsquo;s security, setting up proper auth, etc etc. Quite a hassle for this. So I decided to put tailscale to the test. It\u0026rsquo;s a mesh VPN that allows peers to connect to each other directly using p2p tunnels.\nAll of the clients would have tailscale installed, configured to be on my tailnet, and rules configured to that my sister\u0026rsquo;s phone and the display could talk to the server. Each of them have a static IP in this subnet\nThis would then look like this:\nNow, both my sister\u0026rsquo;s phone and the display can access the frontend exposed by my server, without having to expose it publicly. My sister \u0026lsquo;only\u0026rsquo; have to remember to turn on the VPN whenever she wants to send a message.\nthe display On the display side, it\u0026rsquo;s a simple while loop:\nfetch the API for the latest message sleep like 10 minutes Again, nothing fancy. Doesn\u0026rsquo;t matter if the message isn\u0026rsquo;t received instantaneously:\nThe library for controlling the actual display (https://github.com/waveshareteam/e-Paper) is quite well made. But for some reason the packaging was broken, so I downloaded the package and included it like this waveshare-epaper @ file:///app/waveshare_epaper-1.3.0-py3-none-any.whl in my requirements.\nAnother hurdle I encountered was figuring out how to wrap properly the message if it was too long for a single line. But that has more to do with the font that I used being clearly not monospaced. Fiddling around with arbitrary values to find the exact number of pixels are needed to space things around is always fun !\nWhat\u0026rsquo;s interesting here it that the lib is Pillow based, meaning you construct your image then sends it to the display:\n(you can see my experiment with dark/light mode)\ndef refresh_display(user=\u0026#34;\u0026#34;,date=\u0026#34;\u0026#34;,content=None,dark_theme=False): bg_color = 0 if dark_theme else 255 color = 255 if dark_theme else 0 Limage = Image.new(\u0026#39;1\u0026#39;, (epd.width,epd.height), bg_color) draw = ImageDraw.Draw(Limage) epd.display_Partial(epd.getbuffer(Limage)) # blank bg draw.rectangle((0, 0, epd.width,epd.height), fill = bg_color) # from draw.text((12, 20), user, font = font_35,fill = color) draw.text((12,50),date,font = font_35, fill = color) if isinstance(content,Image.Image): img_w, _ = content.size img_x = int((400 - img_w)/2) Limage.paste(content, (img_x,100)) elif isinstance(content,str): # message line_start_y,line_height = 120,40 lines = textwrap.wrap(content, 18,max_lines=4) for i,l in enumerate(lines): draw.text((12, line_start_y+i*line_height), l, font = font_50, fill = color) epd.display_Partial(epd.getbuffer(Limage)) I played around with various refreshes, because e-ink displays can partially refresh (change only the pixels that changed) or a full (clearing the screen in-between). The partial is quicker, but overtime ghosts of previous images can be visible. So a full refresh is done once in a while.\nFor the date on top the messages, I used the humanize lib to convert epoch timestamps to human-friendly format in French.\nhow it turned out Again, here is the final result:\nQuite pleased with this one. Mixing hardware and software always hits different, and \u0026lsquo;seeing\u0026rsquo; messages being sent literaly across the globe has a very satisfying taste to it.\n","permalink":"/posts/e-paper-frame/","summary":"\u003cp\u003eTransmit paper-like messages from 18000 kms away:\u003c/p\u003e\n\u003cp\u003eA simple, e-ink display that receives messages from loved ones from the other side of the globe.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/e-paper-display/result.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"why\"\u003ewhy\u003c/h1\u003e\n\u003cp\u003eAbout 10 months ago, my sister left our country to do a WHP (Work-Holiday-Program) on the other side of the world (fun fact, this is almost true as the antipodes of France where we live is right next to New-Zealand, where my sister went).\u003c/p\u003e","title":"E Ink Message frame"},{"content":"what \u0026amp; why Every now and then, I have to resize a root partition on an LVM linux machine.\nEvery time, I take more time to recall what steps to do, and in which order. Is it pvresize, then lvrezise, or lvextend ?\nEvery. god. damn. time. Let\u0026rsquo;s put an end to this by having the steps written down somewhere, so I can reference them when the \u0026lsquo;we should increase this vm\u0026rsquo;s disk\u0026rsquo; sentence is heard.\nAlso, how to do it on a non-LVM machine, because sometimes you encounter old, very old VMs that do not use LVM.\nhow The LVM way let\u0026rsquo;s make some assumptions:\nyou are on a fairly recent linux, with the fdisk, df, and the lvm utils installed LVM is already enabled for the partition you have to increase It\u0026rsquo;s a VM and you just changed the disk\u0026rsquo;s size (eg Vcenter, qemu-img..) First, let\u0026rsquo;s check on the VM if we see the new disk size using fdisk:\nroot@debian:~# fdisk -l Disk /dev/vda: 30 GiB, 32212254720 bytes, 62914560 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x8b02b392 Device Boot Start End Sectors Size Id Type /dev/vda1 * 2048 999423 997376 487M 83 Linux /dev/vda2 1001470 41940991 40939522 19.5G 5 Extended /dev/vda5 1001472 41940991 40939520 19.5G 8e Linux LVM Ok, so we do have a 30G disk to play with, and our linux fs is 19.5G. Due to the LVM setup, our partition is at /dev/mapper/debian\u0026ndash;vg-root.\nHere, we have two possible ways of doing things:\nwe either resize the current \u0026lsquo;Extended\u0026rsquo; partition using fdisk, and then we propagate the new size to LVM or we create a new partition from the available space, and add it to our LVM volumes. If you want to use the solution 1, go down and read the section on non-LVM resizing, and apply the steps until the resize2fs. Then come back. Otherwise, tag along:\nFirst, let\u0026rsquo;s create a new partition using fdisk:\nroot@debian:~# fdisk /dev/vda Welcome to fdisk (util-linux 2.38.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. This disk is currently in use - repartitioning is probably a bad idea. It\u0026#39;s recommended to umount all file systems, and swapoff all swap partitions on this disk. Now, we\u0026rsquo;ll create a new partition that will use the new available space:\nCommand (m for help): n Partition type p primary (1 primary, 1 extended, 2 free) l logical (numbered from 5) Select (default p): p Partition number (3,4, default 3): 3 First sector (41940992-62914559, default 41940992): Last sector, +/-sectors or +/-size{K,M,G,T,P} (41940992-62914559, default 62914559): Created a new partition 3 of type \u0026#39;Linux\u0026#39; and of size 10 GiB. We\u0026rsquo;ll change its type to 8e, which is the one for LVM\nCommand (m for help): t Partition number (1-3,5, default 5): 3 Hex code or alias (type L to list all): 8e Changed type of partition \u0026#39;Linux\u0026#39; to \u0026#39;Linux LVM\u0026#39;. Command (m for help): p Disk /dev/vda: 30 GiB, 32212254720 bytes, 62914560 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x8b02b392 Device Boot Start End Sectors Size Id Type /dev/vda1 * 2048 999423 997376 487M 83 Linux /dev/vda2 1001470 41940991 40939522 19.5G 5 Extended /dev/vda3 41940992 62914559 20973568 10G 8e Linux LVM /dev/vda5 1001472 41940991 40939520 19.5G 8e Linux LVM Partition table entries are not in disk order. And we write the partition table:\nCommand (m for help): w The partition table has been altered. Syncing disks. Ok, we now have a new partition of 10G:\nroot@debian:~# fdisk -l Disk /dev/vda: 30 GiB, 32212254720 bytes, 62914560 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x8b02b392 Device Boot Start End Sectors Size Id Type /dev/vda1 * 2048 999423 997376 487M 83 Linux /dev/vda2 1001470 41940991 40939522 19.5G 5 Extended /dev/vda3 41940992 62914559 20973568 10G 8e Linux LVM /dev/vda5 1001472 41940991 40939520 19.5G 8e Linux LVM We\u0026rsquo;ll now create a new volume:\nroot@debian:~# pvcreate /dev/vda3 Physical volume \u0026#34;/dev/vda3\u0026#34; successfully created. root@debian:~# pvs PV VG Fmt Attr PSize PFree /dev/vda3 debian-vg lvm2 a-- 10.00g 10.00g /dev/vda5 debian-vg lvm2 a-- \u0026lt;19.52g 0 We then extend our current VG with this new PV:\nroot@debian:~# vgextend debian-vg /dev/vda3 Volume group \u0026#34;debian-vg\u0026#34; successfully extended root@debian:~# vgs VG #PV #LV #SN Attr VSize VFree debian-vg 2 2 0 wz--n- \u0026lt;29.52g 10.00g And finally, we extend the LV of our root partition with this new PV:\nroot@debian:~# lvextend /dev/debian-vg/root /dev/vda3 Size of logical volume debian-vg/root changed from 18.56 GiB (4752 extents) to 28.56 GiB (7312 extents). Logical volume debian-vg/root successfully resized. root@debian:~# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root debian-vg -wi-ao---- 28.56g swap_1 debian-vg -wi-ao---- 980.00m We can now resize the filesystem of the VG using resize2fs:\nroot@debian:~# resize2fs /dev/mapper/debian--vg-root resize2fs 1.47.0 (5-Feb-2023) Filesystem at /dev/mapper/debian--vg-root is mounted on /; on-line resizing required old_desc_blocks = 3, new_desc_blocks = 4 The filesystem on /dev/mapper/debian--vg-root is now 7487488 (4k) blocks long. A final check to verify that our fs takes all the space:\nroot@debian:~# df -h Filesystem Size Used Avail Use% Mounted on udev 961M 0 961M 0% /dev tmpfs 197M 632K 197M 1% /run /dev/mapper/debian--vg-root 28G 1.6G 26G 6% / tmpfs 984M 0 984M 0% /dev/shm tmpfs 5.0M 0 5.0M 0% /run/lock /dev/vda1 455M 98M 333M 23% /boot tmpfs 197M 0 197M 0% /run/user/1000 And we\u0026rsquo;re done !\nThe non LVM way Disclaimer, this only works if the partition you intend to extend is the last one on the disk.\nIf you read the section on LVM, then the procedure is the same, but without the LVM stuff:\nwe use fdisk to delete, extend, write the new partition, we resize the fs accordingly tadaa it\u0026rsquo;s done. Ok first, check the disk listing:\ncoco@buntuvm:~$ sudo fdisk -l ... Disk /dev/vda: 35 GiB, 37580963840 bytes, 73400320 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: 42E119A3-9BE7-482B-AA89-803A5A7BFDAB Device Start End Sectors Size Type /dev/vda1 2048 4095 2048 1M BIOS boot /dev/vda2 4096 52426751 52422656 25G Linux filesystem Our /dev/vda2 is where our root partition is, and is 25Gig. But the disk is 35Gig, meaning he can extend the partition by around 10 Gig.\nIf we confirm using df, we cam see that our filesystem reports a Size of 25G.\ncoco@buntuvm:~$ sudo df -h Filesystem Size Used Avail Use% Mounted on tmpfs 392M 1.6M 391M 1% /run /dev/vda2 25G 8.9G 15G 39% / tmpfs 2.0G 0 2.0G 0% /dev/shm tmpfs 5.0M 8.0K 5.0M 1% /run/lock tmpfs 392M 108K 392M 1% /run/user/1000 So now we\u0026rsquo;ll use fdisk on our disk (the whole disk, not the partition)\ncoco@buntuvm:~$ sudo fdisk /dev/vda Welcome to fdisk (util-linux 2.39.3). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. GPT PMBR size mismatch (52428799 != 73400319) will be corrected by write. The backup GPT table is not on the end of the device. This problem will be corrected by write. This disk is currently in use - repartitioning is probably a bad idea. It\u0026#39;s recommended to umount all file systems, and swapoff all swap partitions on this disk. We can use \u0026lsquo;p\u0026rsquo; to print the partition table:\nCommand (m for help): p Disk /dev/vda: 35 GiB, 37580963840 bytes, 73400320 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: 42E119A3-9BE7-482B-AA89-803A5A7BFDAB Device Start End Sectors Size Type /dev/vda1 2048 4095 2048 1M BIOS boot /dev/vda2 4096 52426751 52422656 25G Linux filesystem Now, we\u0026rsquo;ll delete the partition number 2 (the Linux filesystem one). Note that this is \u0026lsquo;safe\u0026rsquo; because we are not actually deleting the partition, but rather the information about where the partition starts and where it ends.\nCommand (m for help): d Partition number (1,2, default 2): 2 Partition 2 has been deleted. And now, we\u0026rsquo;ll recreate the partition, with the same start sector, but fdisk will extend the end to the last available sector on our disk, which is \u0026lsquo;10G further\u0026rsquo; than before.\nCommand (m for help): n Partition number (2-128, default 2): First sector (4096-73400286, default 4096): Last sector, +/-sectors or +/-size{K,M,G,T,P} (4096-73400286, default 73398271): Created a new partition 2 of type \u0026#39;Linux filesystem\u0026#39; and of size 35 GiB. Partition #2 contains a ext4 signature. Do you want to remove the signature? [Y]es/[N]o: n If we print the partition table again, we can see that our second partition is now 35G in size.\nCommand (m for help): p Disk /dev/vda: 35 GiB, 37580963840 bytes, 73400320 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: 42E119A3-9BE7-482B-AA89-803A5A7BFDAB Device Start End Sectors Size Type /dev/vda1 2048 4095 2048 1M BIOS boot /dev/vda2 4096 73398271 73394176 35G Linux filesystem Finally, we do not forget to write our changes to the disk:\nCommand (m for help): w The partition table has been altered. Syncing disks. Great. Now the disk has been resized, but the filesystem doesn\u0026rsquo;t know it yet. If we rerun df, we\u0026rsquo;ll see the same 25G listed:\ncoco@buntuvm:~$ sudo df -h Filesystem Size Used Avail Use% Mounted on tmpfs 392M 1.6M 391M 1% /run /dev/vda2 25G 8.9G 15G 39% / tmpfs 2.0G 0 2.0G 0% /dev/shm tmpfs 5.0M 8.0K 5.0M 1% /run/lock tmpfs 392M 108K 392M 1% /run/user/1000 To fix that, we\u0026rsquo;ll use resize2fs that can extend a given ext4 partition. Here, we\u0026rsquo;ll give it the partition path:\ncoco@buntuvm:~$ sudo resize2fs /dev/vda2 resize2fs 1.47.0 (5-Feb-2023) Filesystem at /dev/vda2 is mounted on /; on-line resizing required old_desc_blocks = 4, new_desc_blocks = 5 The filesystem on /dev/vda2 is now 9174272 (4k) blocks long. And if we check the output of df again, we now have 35G to play with !\ncoco@buntuvm:~$ sudo df -h Filesystem Size Used Avail Use% Mounted on tmpfs 392M 1.6M 391M 1% /run /dev/vda2 35G 8.9G 24G 28% / tmpfs 2.0G 0 2.0G 0% /dev/shm tmpfs 5.0M 8.0K 5.0M 1% /run/lock tmpfs 392M 108K 392M 1% /run/user/1000 Hopefully these instructions will serve me well next time (hey future me !)\nRegards, previous me\n","permalink":"/posts/lvm-resize/","summary":"\u003ch1 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h1\u003e\n\u003cp\u003eEvery now and then, I have to resize a root partition on an LVM linux machine.\u003c/p\u003e\n\u003cp\u003eEvery time, I take more time to recall what steps to do, and in which order. Is it \u003ccode\u003epvresize\u003c/code\u003e, then \u003ccode\u003elvrezise\u003c/code\u003e, or \u003ccode\u003elvextend\u003c/code\u003e ?\u003c/p\u003e\n\u003cp\u003eEvery. god. damn. time. Let\u0026rsquo;s put an end to this by having the steps written down somewhere,\nso I can reference them when the \u0026lsquo;we should increase this vm\u0026rsquo;s disk\u0026rsquo; sentence is heard.\u003c/p\u003e","title":"How to resize an LVM ext4 partition (and also non-LVM because old VMs)"},{"content":"what \u0026amp; why Lately (in the last year) I\u0026rsquo;ve been using openAI\u0026rsquo;s chatGPT 3.5-turbo in my terminal, as kind of a personnal rubberducky/sidekick engineer.\nIn my fish config I had the following function (from my dotfiles):\n# needs set -U OPENAI_KEY \u0026lt;KEY\u0026gt; if command -q https; and command -q yq alias h \u0026#39;hey_gpt\u0026#39; function hey_gpt --description \u0026#34;talk to gpt\u0026#34; set prompt (echo $argv | string join \u0026#39; \u0026#39;) set gpt (https -b post api.openai.com/v1/chat/completions \\ \u0026#34;Authorization: Bearer $OPENAI_KEY\u0026#34; \\ model=gpt-3.5-turbo \\ temperature:=0.25 \\ stream:=true \\ messages:=\u0026#39;[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026#39;$prompt\u0026#39;\u0026#34;}]\u0026#39;) for chunk in $gpt if test $chunk = \u0026#39;data: [DONE]\u0026#39; break else if string match -q --regex \u0026#34;content\u0026#34; $chunk yq -0 \u0026#39;.choices[0].delta.content\u0026#39; \u0026lt; (echo -n $chunk | string replace \u0026#39;data: \u0026#39; \u0026#39;\u0026#39; | psub) end end end end This allowed me to do things like this right in my terminal:\n\u0026gt; h how can i extract a single file from a tar archive To extract a single file from a .tar archive, you can use the tar command in the terminal or command line. The basic syntax for extracting a specific file is as follows: bash tar -xf archive.tar path/to/file Here\u0026#39;s a breakdown of the command: tar: The command to call the tar utility. -x: Tells tar to extract files. -f: Specifies that you\u0026#39;re going to specify the name of the archive file. archive.tar: The name of your tar archive. Replace archive.tar with the actual name of your tar file. path/to/file: The path to the file you want to extract from the archive, relative to the root of the tar archive. Replace this with the actual path to your file within the archive. But earlier this month, openAI decided to change their billing policy, and instead of being a \u0026lsquo;pay as you go\u0026rsquo; model, they switched to a \u0026lsquo;pay first, use credits\u0026rsquo;. Well my \u0026lsquo;invoices\u0026rsquo; for openAI were at best a couple of cents every month, and it seemed silly to pay like 2 euros, event if that would last me a couple of years.\nSo let\u0026rsquo;s see how can we reproduce the same setup, but using local AI.\nhow the easiest way of running local LLM AFAIK is through ollama.\nYou just download it and put it in your path and you are basically ready to go.\nSomething like:\n\u0026gt; brew install ollama \u0026gt; ollama serve \u0026amp; \u0026gt; ollama run gemma:7b how far is the sun ? The average distance between the Sun and Earth is approximately: **93 million miles (150 million kilometers)** This distance is known as an **Astronomical Unit (AU)**, which is often used to measure distances within our solar system. Then, we just have to find an appropriate model. In my case, I want a model that can help me mostly for tech related questions, like how to use the command X to do Y, maybe some code/manifests generation.\nWe can define a Modelfile. It\u0026rsquo;s basically a Dockerfile for LLMs. Mine looks like this:\nFROM mistral:7b PARAMETER temperature 0.2 PARAMETER top_k 30 PARAMETER top_p 0.5 SYSTEM \u0026#34;\u0026#34;\u0026#34;You are an experienced software engineer. You have deep knowledge regarding linux, devops tooling, and kubernetes. Your answers should be succint and to the point. do not write more than a few lines. If the user doesn\u0026#39;t ask for an explanation or an example, do not provide one. Just provide the necessary informations, like a shell command, a code snippet. \u0026#34;\u0026#34;\u0026#34; I can now create my own tuned LLM using :\n\u0026gt; ollama create -f Modelfile sidekick And my wrapper has been heavily simplified to this:\nfunction help_me --description \u0026#39;talk to ollama\u0026#39; ollama run sidekick \u0026#34;$argv\u0026#34; end My workflow is now back to normal, and I can ask my sidekick stupid questions about the find / tar arguments daily:\n\u0026gt; h how to find only files older than 10 days in linux using find `find /path -type f -mtime +10` This model consumes around 4Go of RAM when loaded, and it averages 6 tokens/sec for eval:\ntotal duration: 6.938500483s load duration: 165.753µs prompt eval count: 17 token(s) prompt eval duration: 1.347514s prompt eval rate: 12.62 tokens/s eval count: 35 token(s) eval duration: 5.590406s eval rate: 6.26 tokens/s Obviously depending on the machine things might be slower than using openAI\u0026rsquo;s API, and my machine might get a bit warm, but now I know my questions aren\u0026rsquo;t being used for $BIG_CORP_AI, and I can fine tune the behavior and responses of the model !\n\u0026gt; h how to end a post about using an LLM as a sidekick engineer To conclude, as a Sidekick Engineer using an LLM (Large Language Model) in a Linux environment with DevOps tooling and Kubernetes: 1. Install the required packages for your LLM. 2. Set up authentication and access to necessary repositories. 3. Configure your LLM to interact with tools like Git, Docker, Helm, etc. 4. Use your LLM to assist in writing scripts, configuring files, or troubleshooting issues. 5. Ensure proper security measures are in place for sensitive data and access. 6. Continuously improve the effectiveness of your LLM by fine-tuning and updating it. 7. Monitor performance and resource usage to maintain optimal system health. Happy engineering! Oh well, my job ain\u0026rsquo;t dead anytime soon.\nllama3 update With the release of llama3 I\u0026rsquo;ve switched to this model (specifically llama3:8b) and tweaked the system prompt a bit:\nThis is quite an improvement over mistral/gemma. The responses are on point, usually better overall (in accurary / correctness).\nAsking the same question as before:\n\u0026gt; h write the conclusion of a blog post about using a local LLM as a sidekick engineer \u0026#34;In conclusion, leveraging a local Large Language Model (LLM) as a sidekick engineer has revolutionized my workflow and productivity. By automating repetitive tasks, providing instant language translation, and offering expert-level suggestions, my LLM sidekick has become an indispensable tool in my daily engineering routine. With its ability to learn from my interactions and adapt to my specific needs, I\u0026#39;m excited to see the continued evolution of this technology and how it will further augment my skills as a software engineer.\u0026#34; On a more tech subject:\n\u0026gt; h how to copy every file ending in pdf recursively from a folder into another given directory `find /source/folder -type f -name \u0026#34;*.pdf\u0026#34; -exec cp {} /destination/directory \\;` It\u0026rsquo;s still not as fast as chatGPT (and certainly not as good as the 4o model), but it still serves me daily !\n","permalink":"/posts/local-llm/","summary":"\u003ch1 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h1\u003e\n\u003cp\u003eLately (in the last year) I\u0026rsquo;ve been using openAI\u0026rsquo;s chatGPT 3.5-turbo in my terminal, as kind of a personnal rubberducky/sidekick engineer.\u003c/p\u003e\n\u003cp\u003eIn my fish config I had the following function (from \u003ca href=\"https://github.com/k0rventen/.dotfiles\"\u003emy dotfiles\u003c/a\u003e):\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# needs set -U OPENAI_KEY \u0026lt;KEY\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"nb\"\u003ecommand\u003c/span\u003e -q https\u003cspan class=\"p\"\u003e;\u003c/span\u003e and \u003cspan class=\"nb\"\u003ecommand\u003c/span\u003e -q yq\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nb\"\u003ealias\u003c/span\u003e h \u003cspan class=\"s1\"\u003e\u0026#39;hey_gpt\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"k\"\u003efunction\u003c/span\u003e hey_gpt --description \u003cspan class=\"s2\"\u003e\u0026#34;talk to gpt\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"nb\"\u003eset\u003c/span\u003e prompt \u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003eecho\u003c/span\u003e \u003cspan class=\"nv\"\u003e$argv\u003c/span\u003e \u003cspan class=\"p\"\u003e|\u003c/span\u003e string join \u003cspan class=\"s1\"\u003e\u0026#39; \u0026#39;\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"nb\"\u003eset\u003c/span\u003e gpt \u003cspan class=\"o\"\u003e(\u003c/span\u003ehttps -b post api.openai.com/v1/chat/completions \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e                  \u003cspan class=\"s2\"\u003e\u0026#34;Authorization: Bearer \u003c/span\u003e\u003cspan class=\"nv\"\u003e$OPENAI_KEY\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e                  \u003cspan class=\"nv\"\u003emodel\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003egpt-3.5-turbo \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e                  temperature:\u003cspan class=\"o\"\u003e=\u003c/span\u003e0.25 \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e                  stream:\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nb\"\u003etrue\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e                  messages:\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026#39;\u003c/span\u003e\u003cspan class=\"nv\"\u003e$prompt\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;\u0026#34;}]\u0026#39;\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003efor\u003c/span\u003e chunk in \u003cspan class=\"nv\"\u003e$gpt\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"nb\"\u003etest\u003c/span\u003e \u003cspan class=\"nv\"\u003e$chunk\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;data: [DONE]\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"nb\"\u003ebreak\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"k\"\u003eif\u003c/span\u003e string match -q --regex \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e \u003cspan class=\"nv\"\u003e$chunk\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              yq -0 \u003cspan class=\"s1\"\u003e\u0026#39;.choices[0].delta.content\u0026#39;\u003c/span\u003e \u0026lt; \u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003eecho\u003c/span\u003e -n \u003cspan class=\"nv\"\u003e$chunk\u003c/span\u003e \u003cspan class=\"p\"\u003e|\u003c/span\u003e string replace \u003cspan class=\"s1\"\u003e\u0026#39;data: \u0026#39;\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e \u003cspan class=\"p\"\u003e|\u003c/span\u003e psub\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          end\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      end\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  end\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eend\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis allowed me to do things like this right in my terminal:\u003c/p\u003e","title":"Using a local LLM as a personnal engineering sidekick"},{"content":"This is a list of various ressources, blog posts, websites, books that enlightened me at some point:\nbooks links https://registerspill.thorstenball.com/p/which-command-did-you-run-1731-days https://en.wikipedia.org/wiki/Second-system_effect https://www.simplermachines.com/why-you-need-a-wtf-notebook/ ","permalink":"/links/","summary":"\u003cp\u003eThis is a list of various ressources, blog posts, websites, books that enlightened me at some point:\u003c/p\u003e\n\u003ch3 id=\"books\"\u003ebooks\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"links\"\u003elinks\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://registerspill.thorstenball.com/p/which-command-did-you-run-1731-days\"\u003ehttps://registerspill.thorstenball.com/p/which-command-did-you-run-1731-days\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Second-system_effect\"\u003ehttps://en.wikipedia.org/wiki/Second-system_effect\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.simplermachines.com/why-you-need-a-wtf-notebook/\"\u003ehttps://www.simplermachines.com/why-you-need-a-wtf-notebook/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"Links"},{"content":"what and why At $job, we are using jenkins with a dozen or so docker runners, each running their own docker daemon. At least a few hundred builds are carried out daily on these runners, mainly producing docker images that are then pushed onto a testing/production container registry.\nBut two jobs from the same repo might not be using the same CI runner (most of the time they are not). It means each docker runner might need to rebuild the first couple of layers from a given Dockerfile, despite another worker having already built the same layers a few hours/minutes ago. That\u0026rsquo;s wasting CPU cycles, bandwith, and it\u0026rsquo;s also increasing the time each build is taking.\nSo the goal is to find a way for each worker to share its layers to the others, and for everyone to be able to use this system to pull redundant layers that might already exist.\nhow The setup is very simple: We\u0026rsquo;ll use a dedicated docker registry (using the registry image), as a centralised layer cache. Then we\u0026rsquo;ll configure each worker to point to that registry when building an image.\nNow, when a runner builds an image, it will store the intermediary layers in this shared registry, for other runners to use. And when another runner need to build an image with the same starting layers, he\u0026rsquo;ll simply pull from this registry, rather than rebuilding locally the same layers.\nHere is an overview of a PoC setup we\u0026rsquo;ll deploy to test our hypothesis:\nvm1, our docker registry vm2 and vm3, two \u0026lsquo;CI runners\u0026rsquo; (for our purpose, they will only have docker installed and we\u0026rsquo;ll manually launch \u0026lsquo;jobs\u0026rsquo;) To reproduce, you can use anything that can create VMs/containers, like multipass, compose, kvm, virtualbox, distrobox, or even 3 real machines.\nsetup (using multipass) I will be using multipass, but anything that can create a network with multiple VM should do.\nWe\u0026rsquo;ll leverage cloud-init to provision our VM, if your provider supports it, you can copy the content of the following files:\nregistry.yaml\nruncmd: - \u0026#34;curl -fsSL https://get.docker.com -o get-docker.sh\u0026#34; - \u0026#34;sh get-docker.sh\u0026#34; - \u0026#34;docker run -d -p 5000:5000 --name registry registry:latest\u0026#34; worker.yaml\nruncmd: - \u0026#34;curl -fsSL https://get.docker.com -o get-docker.sh\u0026#34; - \u0026#34;sh get-docker.sh\u0026#34; - \u0026#34;usermod -aG docker ubuntu\u0026#34; write_files: - owner: ubuntu:ubuntu path: /home/ubuntu/buildkit.toml permissions: \u0026#39;0644\u0026#39; content: | [registry.\u0026#34;registry:5000\u0026#34;] http = true Now, create 3 VMs using these files:\nmultipass launch -n registry --cloud-init registry.yaml 22.04 multipass launch -n worker1 --cloud-init worker.yaml 22.04 multipass launch -n worker2 --cloud-init worker.yaml 22.04 We can verify that the registry is actually running the container registry:\n\u0026gt; multipass exec registry sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e3e83c6d6c69 registry:latest \u0026#34;/entrypoint.sh /etc…\u0026#34; 2 minutes ago Up 2 minutes 0.0.0.0:5000-\u0026gt;5000/tcp, :::5000-\u0026gt;5000/tcp registry manual setup If deploying manually, once you have the 3 machines setup:\ninstall docker on all of them: start a container registry on our registry machine: docker run -d -p 5000:5000 --name registry registry:latest configure dns so that the workers can access the registry at registry (this is because the buildx backend for docker does not use /etc/hosts), so you might need an external DNS where you can add A records for the registry machine. That\u0026rsquo;s it, we now have a container registry available at registry:5000. It\u0026rsquo;s not production ready, as it\u0026rsquo;s serving over HTTP and has no auth, but for our use-case this will suffice.\nbuilding our first image Now let\u0026rsquo;s create a Dockerfile that we\u0026rsquo;ll execute on one of our worker:\n(multipass shell worker1)\nFROM alpine RUN apk add jq RUN sleep 20 # let\u0026#39;s say we build some boilerplate stuff here RUN echo \u0026#34;$(date)\u0026#34; \u0026gt; /date If we instruct docker to build it:\nworker1:~$ time docker build -t app . [...] =\u0026gt; =\u0026gt; naming to docker.io/library/app real 0m23.446s user 0m0.133s sys 0m0.021s ok, so 23s to build it. Now let\u0026rsquo;s say our second worker has to build the same image (do not forget to copy the Dockerfile on worker2). Running that on our worker2 should take around the same time: (multipass shell worker2):\nworker2:~$ time docker build -t app . [...] =\u0026gt; =\u0026gt; naming to docker.io/library/app real 0m23.449s user 0m0.126s sys 0m0.047s But if we run it again on worker1:\nworker1:~$ time docker build -t app . [...] =\u0026gt; CACHED [2/4] RUN apk add jq =\u0026gt; CACHED [3/4] RUN sleep 20 # let\u0026#39;s say we build some boilerplate stuff here =\u0026gt; CACHED [4/4] RUN echo \u0026#34;$(date)\u0026#34; \u0026gt; /date =\u0026gt; exporting to image =\u0026gt; =\u0026gt; exporting layers =\u0026gt; =\u0026gt; writing image sha256:d902f59380db83b19d90aff37674566688db1895f97c418f7f0a561a368b54d3 =\u0026gt; =\u0026gt; naming to docker.io/library/app real 0m0.889s user 0m0.075s sys 0m0.024s Note the CACHED in some of the log lines on our worker1. That\u0026rsquo;s because the layers for the first 3 instructions of the Dockerfile are already present in our first worker. He\u0026rsquo;s using them rather than building them as he did the first time around.\nconfiguring the cache registry So now, let\u0026rsquo;s make use of our registry and share these layers between runners. So when we update the Dockerfile, runners who haven\u0026rsquo;t yet built this image can take advantage of the registry.\nFor that, we\u0026rsquo;ll use the following arguments to our docker command:\nbuildx: only the buildx backend supports the use of external caching mecanisms as the one we are using here is the buildx documentation --cache-from type=registry,ref=registry.local/image: tell docker to cache the layer we build to this registry --cache-to type=registry,ref=registry.local/image: tell docker to check this registry before building a layler --push or --load: either push the final image to the registry, or load it to the host\u0026rsquo;s docker engine. buildkit config Due to our unsecure setup, we\u0026rsquo;ll have to tell buildx that our registry is using http. On both workers, create a file buildkit.toml:\n[registry.\u0026#34;registry.local\u0026#34;] http = true Then create our builkit with our config:\n\u0026gt; docker buildx create --config=buildkit.toml --use quirky_panini # that\u0026#39;s cute let\u0026rsquo;s finally use the registry We\u0026rsquo;ll change the base image of our Dockerfile, so we know that the first time we build it, we can\u0026rsquo;t use any of the cache we have already. Let\u0026rsquo;s change the base image from alpine to python:\nFROM python:alpine Now let\u0026rsquo;s try to build our image with our shared registry:\nOn worker1:\n\u0026gt; docker buildx build -t registry:5000/image \\ --cache-from type=registry,ref=registry:5000/image \\ --cache-to type=registry,ref=registry:5000/image --load . [+] Building 29.9s (11/11) FINISHED =\u0026gt; [internal] booting buildkit =\u0026gt; =\u0026gt; pulling image moby/buildkit:buildx-stable-1 =\u0026gt; =\u0026gt; creating container buildx_buildkit_amazing_babbage0 =\u0026gt; [internal] load build definition from Dockerfile =\u0026gt; =\u0026gt; transferring dockerfile: 154B =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest =\u0026gt; [internal] load .dockerignore =\u0026gt; =\u0026gt; transferring context: 2B =\u0026gt; ERROR importing cache manifest from registry:5000/image =\u0026gt; [1/4] FROM docker.io/library/python:alpine@sha256:1a0501213b470de000d8432b3caab9d8de5489e9443c2cc7ccaa =\u0026gt; =\u0026gt; resolve docker.io/library/python:alpine@sha256:1a0501213b470de000d8432b3caab9d8de5489e9443c2cc7ccaa =\u0026gt; =\u0026gt; sha256:4abcf20661432fb2d719aaf90656f55c287f8ca915dc1c92ec14ff61e67fbaf8 3.41MB / 3.41MB =\u0026gt; =\u0026gt; extracting sha256:4abcf20661432fb2d719aaf90656f55c287f8ca915dc1c92ec14ff61e67fbaf8 =\u0026gt; [2/4] RUN apk add jq =\u0026gt; [3/4] RUN sleep 20 # let\u0026#39;s say we build some boilerplate stuff here =\u0026gt; [4/4] RUN echo \u0026#34;$(date)\u0026#34; \u0026gt; /date =\u0026gt; exporting to image =\u0026gt; =\u0026gt; exporting layers =\u0026gt; =\u0026gt; exporting manifest sha256:c13b6d9ce9d3d64e17f3443ae9082cf1b9c6e9e07922188900bc9942175fb073 =\u0026gt; =\u0026gt; exporting config sha256:0da8809d7104cf20453a6b2d2276b089f40bfb555e0254db6fa40b0f39aa07ae =\u0026gt; =\u0026gt; exporting attestation manifest sha256:113dfd6d03ed8c503d0b91ef9c69ec6f9c0fb92b9d656062ec3e79ceb9d0a =\u0026gt; =\u0026gt; exporting manifest list sha256:35ba11d8517d1452341090bf6884afd35b595389cf6559988662d76f7e62851d =\u0026gt; =\u0026gt; pushing layers =\u0026gt; =\u0026gt; pushing manifest for registry:5000/image:latest@sha256:35ba11d8517d1452341090bf6884afd35b595389cf65 =\u0026gt; exporting cache to registry =\u0026gt; =\u0026gt; preparing build cache for export =\u0026gt; =\u0026gt; writing layer sha256:4abcf20661432fb2d719aaf90656f55c287f8ca915dc1c92ec14ff61e67fbaf8 =\u0026gt; =\u0026gt; writing layer sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 =\u0026gt; =\u0026gt; writing layer sha256:97a760744e8ba94a04280f62d469902bcb58b128da9f6501db9822ee8ded0a63 =\u0026gt; =\u0026gt; writing layer sha256:cc84016181cd34ccdc572a0a034e46fe491d3a01967328d7370bab371a17c868 =\u0026gt; =\u0026gt; writing config sha256:60802e2ae4cac776269d496cd99bf016a2fd51220214c8736e63914a0eca9ca8 =\u0026gt; =\u0026gt; writing cache manifest sha256:610b85677a133f2ea67eecbbc3ba704e0d3eddf65b48ec0c4293b89d28a3a42b ------ \u0026gt; importing cache manifest from registry:5000/image: real 0m25.925s user 0m0.345s sys 0m0.136s Again, around 25s (there are a few seconds for the buildkit container to boot up). But now, let\u0026rsquo;s do the same thing on the second worker. We\u0026rsquo;ll prune it beforehand, so it has no local cache for what we\u0026rsquo;ll be building:\n\u0026gt; docker system prune -a -f [...] Total reclaimed space: 46.23MB \u0026gt; docker buildx create --config=buildkit.toml --use vibrant_cohen \u0026gt; time docker buildx build -t registry:5000/image \\ --cache-from type=registry,ref=registry:5000/image \\ --cache-to type=registry,ref=registry:5000/image --load . [+] Building 2.4s (11/11) FINISHED docker-container:pensive_taussig =\u0026gt; [internal] load build definition from Dockerfile =\u0026gt; =\u0026gt; transferring dockerfile: 161B =\u0026gt; [internal] load metadata for docker.io/library/python:alpine =\u0026gt; [internal] load .dockerignore =\u0026gt; =\u0026gt; transferring context: 2B =\u0026gt; importing cache manifest from registry:5000/image =\u0026gt; =\u0026gt; inferred cache manifest type: application/vnd.oci.image.index.v1+json =\u0026gt; [1/4] FROM docker.io/library/python:alpine@sha256:1a0501213b470de000d8432b3caab9d8de5489e9443c2cc7cca =\u0026gt; =\u0026gt; resolve docker.io/library/python:alpine@sha256:1a0501213b470de000d8432b3caab9d8de5489e9443c2cc7cca =\u0026gt; CACHED [2/4] RUN apk add jq =\u0026gt; CACHED [3/4] RUN sleep 20 # let\u0026#39;s say we build some boilerplate stuff here =\u0026gt; CACHED [4/4] RUN echo \u0026#34;$(date)\u0026#34; \u0026gt; /date2 =\u0026gt; exporting to docker image format =\u0026gt; =\u0026gt; exporting layers =\u0026gt; =\u0026gt; exporting manifest sha256:4532df521ca93c2519f9ff8338f30e13fba723332447bcd3e003dd47630142a2 =\u0026gt; =\u0026gt; exporting config sha256:9debadcc86872631a1a0b7eafd2972d6beca3456f0b043eb80b52d2681a0d548 =\u0026gt; =\u0026gt; sending tarball =\u0026gt; importing to docker =\u0026gt; =\u0026gt; loading layer d4fc045c9e3a 65.54kB / 3.41MB =\u0026gt; =\u0026gt; loading layer 678cac8b069e 32.77kB / 622.15kB =\u0026gt; =\u0026gt; loading layer 0c9bfb14c909 131.07kB / 11.77MB =\u0026gt; =\u0026gt; loading layer d2968c01735e 242B / 242B =\u0026gt; =\u0026gt; loading layer 5305019f4685 32.77kB / 2.70MB =\u0026gt; =\u0026gt; loading layer 37d2dfc1707b 32.77kB / 2.71MB =\u0026gt; =\u0026gt; loading layer 5f70bf18a086 32B / 32B =\u0026gt; =\u0026gt; loading layer 5a36026cdcc3 126B / 126B =\u0026gt; exporting cache to registry =\u0026gt; =\u0026gt; preparing build cache for export =\u0026gt; =\u0026gt; writing layer sha256:270999341ddcf70feedda4bff6d081483f1ad384e5aa13f268f828ed469f5402 =\u0026gt; =\u0026gt; writing layer sha256:4abcf20661432fb2d719aaf90656f55c287f8ca915dc1c92ec14ff61e67fbaf8 =\u0026gt; =\u0026gt; writing layer sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 =\u0026gt; =\u0026gt; writing layer sha256:4fc96b5c1ba465ba27fb55d4766ade8624de4082ac1530b3293ac735ab3ead50 =\u0026gt; =\u0026gt; writing layer sha256:a8fd6f3f484fdfccf33965ca0f8807e5078a619803cf638d82bc4a405e91de04 =\u0026gt; =\u0026gt; writing layer sha256:caa4e319395ae52ea041b5a6cca32833cecc2b192a18cef42e77a6e0446c9f4a =\u0026gt; =\u0026gt; writing layer sha256:dca80dc46cecdd1a97787a1dd6f74263b9d2f7b0dd3e2e15c109f5e34848c932 =\u0026gt; =\u0026gt; writing layer sha256:fe9e15b6315c34de5c802bdbd343e3ec69bdc4ab870783fc1b9552daaef25e77 =\u0026gt; =\u0026gt; writing config sha256:fccd66ca6f5e29c42a8444b3f74df1ecb8c94114429a851e093de718ba55decc =\u0026gt; =\u0026gt; writing cache manifest sha256:b76d6f554cffd020b6b14656e332527dfb19ab01376d0473cc12a5580a2d9c45 real 0m2.625s user 0m0.255s sys 0m0.041s That was super fast, because for most of the layers, there was a cache hit in the registry. That means whatever layer has been built by another worker, our worker2 can now access it and use it without building it !\nNow if we tweak the Dockerfile a bit, and rerun the command, we might have some cache misses on the new layers, but it\u0026rsquo;s still an improvement !\nWhenever a runner will need these first few layers (eg building dependencies, compiling some boilerplate stuff, etc).. he will be able to pull them from this cache, and only work on what matters (compiling code that has changed, copying over build artifacts..).\nquick maths In this real world very production grade example, we\u0026rsquo;ve reduced the build time from ~20s to ~2s. No matter what the rest of the Dockerfile looks like, it\u0026rsquo;s 18 seconds shaved off the total runtime for this particular job. Let\u0026rsquo;s say these boilerplate layers are used in a repository with 20 commits a day, 5 days a week, we could shave off (18s per run) * (20 runs per day) * 5 days = 30 minutes of CI runtime weekly. Multiply that by the number of repos * weeks worked in a year, and this number can quickly tally up into days !\nlimitations and caveats This setup isn\u0026rsquo;t perfect, and there are a few drawbacks/things to consider when thinking about deploying such a system:\nFirst, it\u0026rsquo;s needless to say, the simple registry:latest container depicted in this setup isn\u0026rsquo;t prepared for much more, as there is no persistent storage, and plain HTTP is used. A more robust container registry might be needed (think Harbor)\nSecond, because this is using buildx, you have to choose between loading the resulting image (eg for testing), or pushing it to the registry, as using both --pull and --load isn\u0026rsquo;t allowed. That might be fine for a (git pull/ docker build/docker push) type of CI, but if you need to both push and use the image, you\u0026rsquo;re out of luck, and you\u0026rsquo;ll surely need to run two commands. Third, and maybe the the main caveat about this setup: cache invalidation:\none of your worker pulls an image, URL or ressource, creates a layer out of it and pushes it to the cache you obviously want to use this layer as much as possible, reducing bandwidth usage, compute time, etc the external ressource gets updated (eg a new commit, updated base image..) But your local instruction for fetching the resource hasn\u0026rsquo;t changed (it\u0026rsquo;s still RUN git clone) you are now out of sync with the resource, building outdated layers. And there lies the root issue, of when should we invalidate a given layer on the registry. Doing it too often kinda defeats the purpose of the shared registry, on the other hand doing it at sparse intervals mean higher chance of using outdated layers.\nThere is no silver bullet for this particular problem, it depends solely on the setup/goal.\nFinally, depending on how many workers/repos/build are used, this can create a huge number of layers in the registry. And all this cache can accumulate quickly, using quite a lot of disk space over time.\nHappy caching !\n","permalink":"/posts/shared-docker-registry-ci/","summary":"\u003ch1 id=\"what-and-why\"\u003ewhat and why\u003c/h1\u003e\n\u003cp\u003eAt $job, we are using jenkins with a dozen or so docker runners, each running their own docker daemon. At least a few hundred builds are carried out daily on these runners, mainly producing docker images that are then pushed onto a testing/production container registry.\u003c/p\u003e\n\u003cp\u003eBut two jobs from the same repo might not be using the same CI runner (most of the time they are not). It means each docker runner might need to rebuild the first couple of layers from a given Dockerfile, despite another worker having already built the same layers a few hours/minutes ago. That\u0026rsquo;s wasting CPU cycles, bandwith, and it\u0026rsquo;s also increasing the time each build is taking.\u003c/p\u003e","title":"Remote layer cache for multiple docker CI runners"},{"content":"A smart city model that I helped build a few years back, to better explain what IoT could mean for a city:\nwhat \u0026amp; why When I arrived at $old_job in 2019, one of my first project was to build a small scale, smart city model that could :\nbe tranported anywhere, be installed on a table easily show what improvements to city life could be made using IoT and be pretty to lure people on our stands ;) The actual model was built by architecture students, with whom I devised a plan to integrate various sensors, LEDs and interactive elements to play with. The whole thing took around a week to be finished.\nThe model has been on display at Cisco Live 2019 and 2024 (that\u0026rsquo;s the picture above), at various events in France (Salon des Maires..) and is now at Axians in La Defense.\nhow Full code is here: https://github.com/k0rventen/smartcity\nHere is a quick diagram showing every major component of the model :\nVarious sensors (temperature, humidity, noise, parking spots..) are connected to Arduinos equipped with LoRa antennas, that transmits to a nearby 4G LoRa gateway. Data \u0026amp; management informations are uploaded to our Acklio server. Actual sensor data is retrieved by an AWS instance for decoding/storage/visualisation. Each section below is describing one of those components.\nHardware listing Here is a list of the hardware components used in the smart city model :\n3 Arduino Uno as micro-controllers, on top of which sits: Grove Hat for easy cable management, with the following sensors : Temperature sensor, Noise sensor, Light sensor, Hall effect sensors, Standard LEDs, Chainable LEDs, Ultrasonic distance sensors, A LoRaWAN antenna, to upload the gathered data to a nearby gateway, A Multitech LoRa gateaway, to receive the payloads from the antenna Each arduino is responsible for a specific task :\nArduino 1 is managing the street lamps and monitoring the temperature / noise level of the city, Arduino 2 is managing the city\u0026rsquo;s trash cans Arduino 3 is managing the parking spots Configuration adjustements The same boilerplate file is deployed to every arduino. The logic is the same everywhere, the arduino fetches the sensors and uploads their data to the cloud through an LoRa gateway. The only difference between each Arduino is which sensors are connected and on which pin.\nThis is the configuration currently is use in the model in header.hpp.\nint UltrasonicSensors[] = {2,3,4}; int HallSensors[] = {6,7,8,2,3,5}; int TemperatureSensor = A2; int SoundSensor = A3; int BrightnessSensor = A1; int FloodSensor = 2; int FloodLED = 3; int WasteLEDs[] = {6,7,8}; int ParkingLEDS[] = {A2,A3,9,4,A0,A1}; int StreetLampsNumber = 8; ChainableLED StreetLamps(4,5, StreetLampsNumber); const int RUNTIME_INTERVAL = 1000; //! time in ms between runs This header.hpp reflects:\nwhich scenarios are being used by this arduino (using the #DEFINE statements) which sensors are connected to the arduino and their positions on the Grove Hat. Scenarios The scenarios are also defined in the .hpp file, so they are only compiled/present on the arduinos that need them. For example the parking scenario, which returns the occupancy for 6 parking spots on the model:\n#ifdef PARKINGSCENARIO /** * @brief Scenario that controls the parking spots of the city. * * Each parking spot is monitored and connected to a LED. * If the parking sport is taken, the LED is up. * */ void ParkingScenario() { for (int i = 0; i \u0026lt; HallSensorsLen; i++) { bool isTaken = !digitalRead(HallSensors[i]); #ifdef DEBUG Serial.print(\u0026#34;\\t\u0026#34;); Serial.print(i); Serial.print(\u0026#34; -\u0026gt; \u0026#34;); Serial.println(isTaken); #endif if (isTaken) { SetLedStatus(ParkingLEDS[i], HIGH); LoRaPayload[i * 2] = \u0026#39;1\u0026#39;; } else { SetLedStatus(ParkingLEDS[i], LOW); LoRaPayload[i * 2] = \u0026#39;0\u0026#39;; } } } #endif Payload structure of the LoRa frame Each arduino has it\u0026rsquo;s own various data to send, so each payload is different :\nGarbage scenario\nByte num 0 1 2 3 4 Desc Trash 1 Null Trash 2 Null Trash 3 Value 1 0 1 0 1 Explanations :\nTrash : 1 means the trashcan is full, 0 means it\u0026rsquo;s not. Parking scenario\nByte num 0 1 2 3 4 5 6 7 8 9 10 Desc Parking 1 Null Parking 2 Null Parking 3 Null Parking 4 Null Parking 5 Null Parking 6 Value 1 0 1 0 0 0 1 0 0 0 1 Explanations :\nParking : 1 means the parking spot is taken, 0 means it\u0026rsquo;s free. Street lamps \u0026amp; metrics scenario\nByte num 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Desc Lights status Null Null Flood status Null Null Null Null Null Null Light level Light level Noise Noise Temp Temp Value 1 0 0 1 0 0 0 0 0 0 8 9 4 8 2 5 Explanations :\nLights status : 1 means the lights are on, 0 means off. Flood status : 1 means flood detected, 0 means no flood detected Light level : Bytes 10 and 11 are forming a numnber reflecting the percentage of light perceived by the sensor, here 89%. Noise : Bytes 12 and 13 are the current noise level reading in decibels. Here is 48 dB. Temp : Bytes 14 and 15, the current temperature reading in Celsius, here 25°C. cloud platform A \u0026lsquo;cloud\u0026rsquo; platform gathers the data sent by the model through a LoRa GW. It\u0026rsquo;s a simple python worker that translates MQTT messages to influxDB. The latter is used by Grafana to display \u0026rsquo;live\u0026rsquo; and historical data:\n","permalink":"/posts/smartcity/","summary":"\u003cp\u003e\u003cem\u003eA smart city model that I helped build a few years back, to better explain what IoT could mean for a city:\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"demo\" loading=\"lazy\" src=\"/smartcity/smartcity-cl24.jpeg\"\u003e\u003c/p\u003e\n\u003ch1 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h1\u003e\n\u003cp\u003eWhen I arrived at $old_job in 2019, one of my first project was to build a small scale, smart city model that could :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebe tranported anywhere,\u003c/li\u003e\n\u003cli\u003ebe installed on a table easily\u003c/li\u003e\n\u003cli\u003eshow what improvements to city life could be made using IoT\u003c/li\u003e\n\u003cli\u003eand be pretty to lure people on our stands ;)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe actual model was built by architecture students, with whom I devised a plan to integrate various sensors, LEDs and interactive elements to play with.\nThe whole thing took around a week to be finished.\u003c/p\u003e","title":"A smart city demonstrator"},{"content":"An \u0026lsquo;almost 1 month-battery-life-e-ink-display-watch\u0026rsquo; that you can build/program.\nwhat \u0026amp; why This summer I came across this project: https://sqfmi.com/watchy/. It\u0026rsquo;s a watch with an e-ink display, and a esp32 based platform that you can build upon. And you even have to build it yourself once it arrives in the mail ! And it\u0026rsquo;s \u0026ldquo;open source hardware and software\u0026rdquo; !\nSo i ordered one, and played with it when it arrived. The websites claims you can even do OTA update to the watch faces from your phone ! Disclaimer, I never managed to make this feature work. But it\u0026rsquo;s a very cool project nonetheless.\nI only need the timekeeping feature of the watch, so the additional capabilites like step counter and weather were a bit useless for my taste, so I decided to dive in the repo. The documentation is very limited, quite sparse, and the only real up-tu-date documentation source is their discord. So I cherry picked what I needed from the OG repo, fork\u0026rsquo;d it, and made my own !\nWith the default code and setup, I could manage at most 6 days before the watch died. The goal was then to increase this as much as possible.\nhow The code is completly rearchitected, and the face isn\u0026rsquo;t decoupled from the logic anymore (as a single watch face was enough).\nI then stripped every code that wasn\u0026rsquo;t useful, disabled everything unecessary (eg the step counter). I also removed as much dependencies as possible, and switched the NTP based time sync feature with an API call to the worldtimeapi:\nif (settings.timezone == \u0026#34;ip\u0026#34;){ weatherQueryURL = \u0026#34;http://worldtimeapi.org/api/ip\u0026#34;; } else{ weatherQueryURL= \u0026#34;http://worldtimeapi.org/api/timezone/\u0026#34; + settings.timezone; } http.begin(weatherQueryURL.c_str()); int httpResponseCode = http.GET(); if (httpResponseCode == 200) { String payload = http.getString(); JSONVar responseObject = JSON.parse(payload); int epoch = int(responseObject[\u0026#34;unixtime\u0026#34;]); int dst_offset = int(responseObject[\u0026#34;dst_offset\u0026#34;]); int tz_offset = int(responseObject[\u0026#34;raw_offset\u0026#34;]); int local_epoch = epoch + dst_offset + tz_offset; tmElements_t tm; breakTime((time_t)local_epoch, tm); RTC.set(tm); ... This is even better than the NTP based one because you can specify ip as your timezone, and the API will guess your local zone and respond with the proper local time.\nFinally I added a sleep mode that will put the watch in deepsleep between user-defined hours (eg 23:00 to 07:00). And it will even vibrate (like an Apple Watch) if you\u0026rsquo;ve configured a wake up time:\n// vibrate if this is alarm time if (currentTime.Hour == settings.alarmHour \u0026amp;\u0026amp; currentTime.Minute == settings.alarmMinute) { vibMotor(75, 4); } // set sleep mode if (currentTime.Hour == settings.bedTimeHour \u0026amp;\u0026amp; currentTime.Minute == settings.bedTimeMinute) { showSleep(); } All of these tweaks together bring the battery life to around 25 days, with a time sync every week. Tinkering a bit with some variables could land a whole month, eg by increasing the sleep period, or syncing the time only once when the watch is plugged in.\n","permalink":"/posts/watchy/","summary":"\u003cp\u003eAn \u0026lsquo;almost 1 month-battery-life-e-ink-display-watch\u0026rsquo; that you can build/program.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"watch\" loading=\"lazy\" src=\"/watchy/watchy.jpg\"\u003e\u003c/p\u003e\n\u003ch1 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h1\u003e\n\u003cp\u003eThis summer I came across this project: \u003ca href=\"https://sqfmi.com/watchy/\"\u003ehttps://sqfmi.com/watchy/\u003c/a\u003e. It\u0026rsquo;s a watch with an e-ink display, and a esp32 based platform that you can build upon. And you even have to build it yourself once it arrives in the mail ! And it\u0026rsquo;s \u0026ldquo;open source hardware and software\u0026rdquo; !\u003c/p\u003e\n\u003cp\u003eSo i ordered one, and played with it when it arrived. The websites claims you can even do OTA update to the watch faces from your phone ! Disclaimer, I never managed to make this feature work. But it\u0026rsquo;s a very cool project nonetheless.\u003c/p\u003e","title":"An E-Ink watch"},{"content":"what \u0026amp; why a bit of context At $dayjob, our backend stack is 99% python based, and deployed through containers on k8s clusters. That means that a lot of time spent in our CI is downloading and installing librairies and modules from Pypi, the Python Package Index.\nFor the sake of data sovereignty and integrity, our CI workers are deployed on-prem, using Gitlab Runners. But due to the variety of projects, we are depending on hundreds of various libraries, ranging from fastapi/flask and their related libraries, to more esoteric ones like pysnmp, yang, scrapli, netmiko/paramiko..\nthe problem That leaves us with a significant amount of bandwith \u0026lsquo;dedicated\u0026rsquo; to just downloading libraires, some of them being quite heavy (ML/CV like torch or openCV are more than 100Mb in size). And besides our CI workers, our dev envs are also generating containers to use in our test envs. And keeping up with recent versions means downloading even more.\nOur internet provider does not have a gigabit-class fiber for where we are (at least not at a reasonable price), so we are stuck on a ~100Mb/s downlink, shared across 2 companies, and dozens of employees.\nNote: While we use the docker layer system in our CI to cache layers between runs, the cache is not shared across runners, and has to be purged regularly (weekly) to avoid disk issues, lingering layers..\nThis combination produce the following: In a CI stage building a container taking up 90s, the download of the libraries can some time tally up to 30, even 40 seconds. Overall, around 50% of our CI time is just downloading.\nFurthermore, It seems quite wasteful to download again and again the same package from PYPI, putting unnecessary strain on our internet connection, their infrastructure, and everything in between.\nSo the goal is to avoid as much as possible the re-download of packages from internet, and improve the time needed to download heavy packages.\nhow What we\u0026rsquo;ve deployed is a docker image that acts as a PIP Proxy on the whole LAN, caching packages as they come and serving them locally when asked again.\nThis approach only required a single change in our CI workflow, which meant 0 downtime when deploying it. And everyone on the network can benefit from them if they add a new env var to their environment.\nBefore digging into the solution, there is some downsides to that solution:\nThat adds a new potential point of failure for our CI. If for some reason the proxy isn\u0026rsquo;t available, pip install will not redirect automatically to the default index URL, and all workflows will be broken The cache size can increase rapidely, so it\u0026rsquo;s important to keep an eye on that before running into disk issues (leading to problem n°1) Developers using the proxy on a mobile laptop will encounter problems when the laptop isn\u0026rsquo;t on the same network as the proxy Now onto the actual solution.\nthe proxy/cache container The docker container is a very simple nginx based image, with the following config from this gist:\nnginx.conf # Loosely based on the following: # (note these do not work correctly in 2023) # https://joelkleier.com/blog/2018-04-17-pypi-temporary-cache.html # https://gist.github.com/dctrwatson/5785638#file-nginx-conf # It\u0026#39;s also very easy to end up not proxying requests; tests/mitmtest.sh should help verify that # pip installs actually avoid hitting upstream error_log /var/log/nginx/error.log; pid /var/run/nginx.pid; worker_processes auto; events { worker_connections 2048; } http { include /etc/nginx/mime.types; default_type application/octet-stream; sendfile on; tcp_nodelay on; tcp_nopush off; reset_timedout_connection on; server_tokens off; gzip on; gzip_types application/vnd.pypi.simple.v1+json; gzip_proxied any; gzip_vary on; log_format pypi_cache \u0026#39;$remote_addr - $host [$time_local] \u0026#39; \u0026#39;request_time=$request_time upstream_time=$upstream_response_time \u0026#39; \u0026#39;cache_status=$upstream_cache_status \\t\u0026#39; \u0026#39;$status \u0026#34;$request\u0026#34; $body_bytes_sent\u0026#39;; access_log /dev/stdout pypi_cache buffer=64k flush=1s; # Log to file, can be useful for dev # access_log /var/log/nginx/cache.log pypi_cache buffer=64k flush=1s; # Cache 50G worth of packages for up to 6 months proxy_cache_path /var/lib/nginx/pypi levels=1:2 keys_zone=pypi:16m inactive=6M max_size=50G; # Having the same upstream server listed twice allegedly forces nginx to retry # connections and not fail the request immediately. upstream sg_pypi { server pypi.org:443; server pypi.org:443; keepalive 16; } upstream sg_pythonhosted { server files.pythonhosted.org:443; server files.pythonhosted.org:443; keepalive 16; } server { listen 80 default_server; proxy_cache pypi; proxy_cache_key $uri/$http_accept_encoding; proxy_cache_lock on; proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504; proxy_http_version 1.1; proxy_ssl_server_name on; # sub_filter can\u0026#39;t apply to gzipped content, so be careful about that add_header X-Pypi-Cache $upstream_cache_status; sub_filter \u0026#39;https://pypi.org\u0026#39; $scheme://$host; sub_filter \u0026#39;https://files.pythonhosted.org/packages\u0026#39; $scheme://$host/packages; sub_filter_once off; sub_filter_types application/vnd.pypi.simple.v1+json application/vnd.pypi.simple.v1+html; location / { proxy_set_header Connection \u0026#34;\u0026#34;; proxy_set_header Accept-Encoding \u0026#34;\u0026#34;; proxy_cache_valid any 5m; proxy_cache_valid 404 1m; proxy_set_header Host pypi.org; proxy_ssl_name pypi.org; proxy_pass https://sg_pypi; proxy_redirect \u0026#39;https://pypi.org\u0026#39; $scheme://$host; } location ^~ /simple { proxy_set_header Connection \u0026#34;\u0026#34;; proxy_set_header Accept-Encoding \u0026#34;\u0026#34;; proxy_cache_valid any 5m; proxy_cache_valid 404 1m; proxy_set_header Host pypi.org; proxy_pass https://sg_pypi; proxy_redirect \u0026#39;https://pypi.org\u0026#39; $scheme://$host; } location ^~ /packages { proxy_set_header Connection \u0026#34;\u0026#34;; proxy_set_header Accept-Encoding \u0026#34;\u0026#34;; proxy_cache_valid any 1M; proxy_cache_valid 404 1m; proxy_set_header Host files.pythonhosted.org; proxy_ssl_name files.pythonhosted.org; proxy_pass \u0026#39;https://sg_pythonhosted/packages\u0026#39;; proxy_redirect \u0026#39;https://files.pythonhosted.org/packages\u0026#39; $scheme://$host/packages; } } } The Dockerfile is simply the aforementionned conf applied on top of a Nginx container:\nDockerfile FROM nginx:latest RUN mkdir -p /var/lib/nginx/pypi/ /var/log/nginx/ /var/run/ ADD nginx.conf /etc/nginx/nginx.conf Create the container image with docker build -t pip_proxy .. Then create a volume for storing the cached packages docker volumes create pip-cache-data. Then start a container:\ndocker run -d -p 80:80 -v pip-cache-data:/var/lib/nginx/pypi/ pip_proxy If you prefer to deploy this on a Kubernetes cluster :\nk create deployment pip-proxy --image \u0026lt;image_tag\u0026gt; --port 80 k expose deployment pip-proxy --port 80 --target-port 80 k create ingress pip-proxy --rule=\u0026#34;pip.local.domain.fr/*=pip-proxy:80\u0026#34; telling pip to use our proxy From pip3 install --help:\nPackage Index Options: -i, --index-url \u0026lt;url\u0026gt; Base URL of the Python Package Index (default https://pypi.org/simple). This should point to a repository compliant with PEP 503 (the simple repository API) or a local directory laid out in the same format. --trusted-host \u0026lt;hostname\u0026gt; Mark this host or host:port pair as trusted, even though it does not have valid or any HTTPS. This means we can redirect to another package index, using either the -i argument, or through the PIP_INDEX_URL env var. In case the proxy is using self-signed or non-valid certificates, you can use --trusted-host or the PIP_TRUSTED_HOST env var.\nchanges to the CI In our Gitlab group, under Settings \u0026gt; CI/CD \u0026gt; Variables, we can configure these env vars across all our pipelines:\nHere is an excerpt of a CI stage installing torch:\nBefore, using directly the PYPI repo:\n\u0026gt; time pip install --no-cache torch Collecting torch Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 619.9/619.9 MB 11.4 MB/s eta 0:00:00 ... real\t5m26.790s user\t1m39.378s sys\t0m38.067s After, using the proxy:\nPIP_INDEX_URL=https://pip.local.domain.fr/simple PIP_TRUSTED_HOST=pip.local.domain.fr \u0026gt; pip install --no-cache torch Looking in indexes: https://pip.local.domain.fr/simple Collecting torch Downloading http://pip.local.domain.fr/packages/8c/4d/17e07377c9c3d1a0c4eb3fde1c7c16b5a0ce6133ddbabc08ceef6b7f2645/torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 619.9/619.9 MB 89.7 MB/s eta 0:00:00 ... real\t2m11.356s user\t1m20.170s sys\t0m32.175s We can see that on the second run, pip downloaded torch from our proxy, leading to an ~8x faster download speed. Some packages weren\u0026rsquo;t cached, but next time they are being downloaded, our proxy will serve them from its cache, saving even more time.\nFor that particular example, we\u0026rsquo;ve shaved off more than 3 minutes, or an improvement of ~2.5x time wise ! This is a best-case scenario, with heavy packages, but we\u0026rsquo;ve seen gains across the board.\nI haven\u0026rsquo;t been able to measure how many MB (or maybe GB) of packages we haven\u0026rsquo;t downloaded from PYPI, and therefore how much bandwith we saved, but after only 2 weeks of using the cache, it is now weighting in at around 3.2GB. Speculating that we\u0026rsquo;ve downloaded the same version of most of these packages at least a few more times, it\u0026rsquo;s safe to say that we saved tens of GB !\nroot@pip-proxy-54789c97df-jsg95:/# du -sh /var/lib/nginx/pypi/ 3.2G\t/var/lib/nginx/pypi/ Overall, the solution was deployed in less than a few hours. It\u0026rsquo;s now \u0026lsquo;battletested\u0026rsquo; across hundreds of pipelines, has saved us hours of CI time and avoided re-re-re-downloading tens of gigabytes worth of packages from PYPI. Hopefully we\u0026rsquo;ll never have any issues with it !\n(new post on our CI being broken incoming..)\n","permalink":"/posts/a-pip-proxy/","summary":"\u003ch2 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h2\u003e\n\u003ch3 id=\"a-bit-of-context\"\u003ea bit of context\u003c/h3\u003e\n\u003cp\u003eAt $dayjob, our backend stack is 99% python based, and deployed through containers on k8s clusters. That means that a lot of time spent in our CI is downloading and installing librairies and modules from \u003ca href=\"https://pypi.org\"\u003ePypi\u003c/a\u003e, the Python Package Index.\u003c/p\u003e\n\u003cp\u003eFor the sake of data sovereignty and integrity, our CI workers are deployed on-prem, using Gitlab Runners. But due to the variety of projects, we are depending on hundreds of various libraries, ranging from fastapi/flask and their related libraries, to more esoteric ones like pysnmp, yang, scrapli, netmiko/paramiko..\u003c/p\u003e","title":"A PYPI cache proxy to improve our CI workflows"},{"content":"Story time A funny thing happened today.\nA remote Raspberry Pi deployed a few years back for $client was having issues connecting with another system. When setting up the Pi, I thought of sharing my ssh key from my laptop so I could SSH into it passwordless-style.\nBut it didn\u0026rsquo;t occured to younger (and stupider apparently) me to save the password for that account somewhere. And on that system (and generally on Debian I believe), standard accounts are in the sudo group, but not with passwordless access, meaning you can run sudo but you have to enter your session\u0026rsquo;s password.\nSo here I am, without my own password, having to perform sudo enabled commands:\ncoco@insight-probe-pop:/$ sudo timedatectl show-timesync [sudo] password for coco: Sorry, try again. [sudo] password for coco: Sorry, try again. [sudo] password for coco: sudo: 3 incorrect password attempts coco@insight-probe-pop:/$ fjeziofgjizrejfgzr No f*ckin idea of what the password is. Younger me was smart and surely used a randomly generated password, \u0026ldquo;for the sake of security\u0026rdquo;, obviously.\nI was preparing my email to $client, creating some subtle excuses, when I realized that the Pi was running docker containers. And my account was able to run docker commands, because back when setting the system up, I also added myself to the docker group using usermod -aG docker $USER (on that, it\u0026rsquo;s not recommended, see below on what should be done now using rootless setups).\nAnd through docker you can run privileged containers, right ? Running as root, right ? And mount host files into a container ?\nSO if I could launch a privileged container, and have a user with uid=0, and mount the /etc/passwd file in it, could it be possible to change my password using passwd ?\nOnly one way to find out:\n# start a container mapping the passwd file docker run -it --privileged -v /etc/passwd:/etc/passwd debian root@555b0af3924a:/# # okay the file is here root@555b0af3924a:/# ls -alh /etc/passwd -rw-r--r-- 1 root root 1.3K Jun 12 2020 /etc/passwd # use the container\u0026#39;s passwd binary to change the host\u0026#39;s passwd file root@555b0af3924a:/# passwd coco New password: Retype new password: passwd: password updated successfully root@555b0af3924a:/# exit And now if I try again from my account on that machine:\ncoco@insight-probe-pop:/$ sudo echo boom [sudo] password for coco: boom GOD DAMN IT WORKS !!\nI still haven\u0026rsquo;t figured out if that was a 200IQ move, or just fixing a previous -200IQ move.\nBut I was then able to fix the problem and sent a happy email. Also current me learned something and saved that password somewhere. Today\u0026rsquo;s a good day.\nNote: A lot changed since setting up this system regarding docker and how it handles root access. It even has a rootless mode (doc here), and the same goes for podman (doc). Also giving non-admin users access to the docker daemon isn\u0026rsquo;t a bright idea. Shame on younger me. But also thanks younger me, without that mistake I would still be figuring out how to regain access to that system.\n","permalink":"/posts/reset-password-with-docker/","summary":"\u003ch2 id=\"story-time\"\u003eStory time\u003c/h2\u003e\n\u003cp\u003eA funny thing happened today.\u003c/p\u003e\n\u003cp\u003eA remote Raspberry Pi deployed a few years back for $client was having issues connecting with another system.\nWhen setting up the Pi, I thought of sharing my ssh key from my laptop so I could SSH into it passwordless-style.\u003c/p\u003e\n\u003cp\u003eBut it didn\u0026rsquo;t occured to younger (and stupider apparently) me to save the password for that account somewhere.\nAnd on that system (and generally on Debian I believe), standard accounts are in the sudo group, but not with passwordless access, meaning you can run \u003cem\u003esudo\u003c/em\u003e but you have to enter your session\u0026rsquo;s password.\u003c/p\u003e","title":"Reset a lost account password using docker"},{"content":"A fleet of remote development environments (with docker, fish shell, and even minikube) running in your kubernetes cluster, accessible through VS Code in the browser !\nThere should have been a video here but your browser does not seem to support it. What \u0026amp; why This setup is the v2 of a previous post on remote dev env using jupyterlab I made a year and a half ago. Thee OG setup was functionnal, but it had some issues, mainly around user management, container lifecycle and persistent data handling. As $dayjob has grown, so has the infrastructure, and so has the development needs. So a new solution was required.\nA lot of them exists right now for remote environments, from providers like Github Codespaces, Gitpod, or even DevPod. But the folks at Coder released coder v2 a while back, and that\u0026rsquo;s what I\u0026rsquo;ve used for managing our team\u0026rsquo;s environments since late 2022.\nThe devs needs haven\u0026rsquo;t changed a lot since the first post. Our workflow is cluster-centric, based on skaffold to redeploy our built-on-the-fly-containers as pods directly onto the cluster.\nHow The stack consists of these parts:\na docker image that will be used as the base for our remote envs a kubernetes cluster, which will host everything, the coder platform deployed on said cluster, a custom kubernetes provider for running our docker image inside coder, handling some specific needs we have with our dev envs The Base Image The idea of the base image is to bake everything needed directly into it: vscode, git, fish shell, docker (running in a Docker-in-Docker fashion). I\u0026rsquo;ve already built this image, available at k0rventen/code, but if you want to tweak it, you\u0026rsquo;ll find the necessary files below:\nWe\u0026rsquo;re using an Ubuntu base, plus:\nwe install basic dev utils and requirements for running dockerd we copy the dockerd stuff from their own image we install code-server we change the default shell to fish and copy over our base config files we start a bootstrap.fish script # base image using ubuntu FROM ubuntu:23.04 # install utils (fish shell, ssh) RUN apt update \u0026amp;\u0026amp; apt install -y --no-install-recommends curl ca-certificates git iptables fuse-overlayfs dnsutils less fish openssh-client \u0026amp;\u0026amp; apt clean #install code-server RUN curl -fsSL https://code-server.dev/install.sh | sh -s -- --version 4.14.0 \u0026amp;\u0026amp; rm -rvf /root/.cache # copy dockerd binaries from the docker image COPY --from=docker:20-dind /usr/local/bin/ /usr/local/bin/ # shell config RUN chsh -s /usr/bin/fish COPY config/ /tmp/code # run our launch script ENTRYPOINT [\u0026#34;fish\u0026#34;, \u0026#34;/tmp/code/bootstrap.fish\u0026#34;] The bootstrap.fish has the following duties:\nmake sure mandatory directories are here install Linuxbrew if not present (which will run only once, during the first startup of the env) start the dockerd daemon in the background then start code-server Here is its content: bootstrap.fish\ncd $home mkdir -p projects .config/fish if test ! -e .config/fish/config.fish echo \u0026#34;copying fish config\u0026#34; cp -v /tmp/code/config.fish .config/fish/config.fish end if test ! -e /home/linuxbrew/.linuxbrew/bin echo \u0026#34;installing linuxbrew\u0026#34; bash -c \u0026#39;NONINTERACTIVE=1 /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34;\u0026#39; \u0026amp; end echo \u0026#34;starting dockerd..\u0026#34; sh /usr/local/bin/dockerd-entrypoint.sh \u0026amp; echo \u0026#34;starting code server\u0026#34; exec code-server --bind-addr 0.0.0.0:9069 --auth none --disable-telemetry --disable-update-check projects The config.fish is a very minimal config to get us started:\nconfig.fish\n# quiet fish set fish_greeting set -gx HOMEBREW_NO_ENV_HINTS 1 set -gx HOMEBREW_NO_INSTALL_CLEANUP 1 # brew fish fish_add_path /home/linuxbrew/.linuxbrew/bin # simple fish function fish_prompt printf \u0026#39;\\n%s[%s]%s \u0026gt; \u0026#39; (set_color cyan) (prompt_pwd) (set_color normal) end The Coder platform Coder can be installed on a lot of platform, including docker, k8s and friends. Here we\u0026rsquo;ll concentrate on Kube. Requirements are a cluster with a storage class and an Ingress controller. You\u0026rsquo;ll need helm as well.\nFrom this point forward, I\u0026rsquo;ll assume that you have a cluster which can be accessed using the domain names coder.org and *.coder.org (You can add them in your local DNS server, or as entries in your /etc/hosts file wildcards aren\u0026rsquo;t supported, but for testing purposes you could write the required subdomains as needed).\nDepending on the setup you\u0026rsquo;ll need to adjust some variables in the files below.\nThe configuration of the platform is done through the values.yaml passed to helm. The important bit is the CODER_ACCESS_URL and CODER_WILDCARD_ACCESS_URL env vars and ingress config. They will define how clients can access the platform and their envs.\nvalues.yaml\ncoder: env: - name: CODER_PG_CONNECTION_URL valueFrom: secretKeyRef: name: coder-db-url key: url - name: CODER_ACCESS_URL value: \u0026#34;https://coder.org\u0026#34; - name: CODER_TELEMETRY value: \u0026#34;false\u0026#34; - name: CODER_WILDCARD_ACCESS_URL value: \u0026#34;*.coder.org\u0026#34; - name: CODER_AGENT_URL value: \u0026#34;http://coder\u0026#34; service: type: ClusterIP ingress: enable: true host: \u0026#34;coder.org\u0026#34; wildcardHost: \u0026#34;*.coder.org\u0026#34; annotations: nginx.ingress.kubernetes.io/enable-cors: \u0026#34;true\u0026#34; tls: enable: true Now we can deploy everything:\n# create a namespace for our platform kubectl create ns coder kubectl config set-context --current --namespace coder # postgres will be used by coder helm repo add bitnami https://charts.bitnami.com/bitnami helm install coder-db bitnami/postgresql --namespace coder --set auth.username=coder --set auth.password=coder --set auth.database=coder --set persistence.size=10Gi # create a secret for coder holding the db creds kubectl create secret generic coder-db-url --from-literal=url=\u0026#34;postgres://coder:coder@coder-db-postgresql.coder.svc.cluster.local:5432/coder?sslmode=disable\u0026#34; # deploy coder helm repo add coder-v2 https://helm.coder.com/v2 helm upgrade --install coder coder-v2/coder --namespace coder --values values.yaml You should be able to access the management interface at https://coder.org. Create your admin user there and come back.\nThe custom Kubernetes provider We now need to register a provider for our environments. The default Kubernetes provider available is a good start, but we\u0026rsquo;ll tweak it a bit to our needs. It\u0026rsquo;s a single Terraform file defining the ressources to be created. It\u0026rsquo;s quite long but the gist is that each environment will be composed of:\n1 Pod that will execute our dev env, with configurable ressources allocations (CPU \u0026amp; RAM) 3 PersistantVolumeClaims one for our home folder, mounted on /root one for the dockerd daemon files, on /var/lib/docker one for linuxbrew at /home/linuxbrew The file is quite long: main.tf terraform { required_providers { coder = { source = \u0026#34;coder/coder\u0026#34; version = \u0026#34;~\u0026gt; 0.7.0\u0026#34; } kubernetes = { source = \u0026#34;hashicorp/kubernetes\u0026#34; version = \u0026#34;~\u0026gt; 2.18\u0026#34; } } } provider \u0026#34;coder\u0026#34; { feature_use_managed_variables = true } variable \u0026#34;namespace\u0026#34; { type = string description = \u0026#34;The Kubernetes namespace to create workspaces in (must exist prior to creating workspaces)\u0026#34; default = \u0026#34;coder\u0026#34; } data \u0026#34;coder_parameter\u0026#34; \u0026#34;cpu\u0026#34; { name = \u0026#34;cpu\u0026#34; display_name = \u0026#34;CPU\u0026#34; description = \u0026#34;The number of CPU cores\u0026#34; default = \u0026#34;2\u0026#34; icon = \u0026#34;/icon/memory.svg\u0026#34; mutable = true option { name = \u0026#34;Light machine (2 Cores)\u0026#34; value = \u0026#34;2\u0026#34; } option { name = \u0026#34;Heavy Machine (8 Cores)\u0026#34; value = \u0026#34;8\u0026#34; } } data \u0026#34;coder_parameter\u0026#34; \u0026#34;memory\u0026#34; { name = \u0026#34;memory\u0026#34; display_name = \u0026#34;Memory\u0026#34; description = \u0026#34;The amount of memory in GB\u0026#34; default = \u0026#34;2\u0026#34; icon = \u0026#34;/icon/memory.svg\u0026#34; mutable = true option { name = \u0026#34;2 GB\u0026#34; value = \u0026#34;2\u0026#34; } option { name = \u0026#34;8 GB\u0026#34; value = \u0026#34;8\u0026#34; } } data \u0026#34;coder_parameter\u0026#34; \u0026#34;image\u0026#34; { name = \u0026#34;Image\u0026#34; display_name = \u0026#34;Container Image\u0026#34; description = \u0026#34;The base container image to use\u0026#34; default = \u0026#34;k0rventen/code:0.1\u0026#34; icon = \u0026#34;/icon/memory.svg\u0026#34; mutable = true type = \u0026#34;string\u0026#34; } provider \u0026#34;kubernetes\u0026#34; { # Authenticate via ~/.kube/config or a Coder-specific ServiceAccount, depending on admin preferences config_path = null } data \u0026#34;coder_workspace\u0026#34; \u0026#34;me\u0026#34; {} resource \u0026#34;coder_agent\u0026#34; \u0026#34;main\u0026#34; { os = \u0026#34;linux\u0026#34; arch = \u0026#34;amd64\u0026#34; startup_script_timeout = 180 startup_script = \u0026lt;\u0026lt;-EOT set -e fish /tmp/code/bootstrap.fish EOT } # code-server resource \u0026#34;coder_app\u0026#34; \u0026#34;code-server\u0026#34; { agent_id = coder_agent.main.id slug = \u0026#34;code-server\u0026#34; display_name = \u0026#34;code-server\u0026#34; icon = \u0026#34;/icon/code.svg\u0026#34; url = \u0026#34;http://localhost:9069?folder=/root/projects\u0026#34; subdomain = false share = \u0026#34;owner\u0026#34; healthcheck { url = \u0026#34;http://localhost:9069/healthz\u0026#34; interval = 3 threshold = 10 } } resource \u0026#34;kubernetes_persistent_volume_claim\u0026#34; \u0026#34;home\u0026#34; { metadata { name = \u0026#34;coder-${lower(data.coder_workspace.me.owner)}-${lower(data.coder_workspace.me.name)}-home\u0026#34; namespace = var.namespace labels = { \u0026#34;app.kubernetes.io/name\u0026#34; = \u0026#34;coder-pvc\u0026#34; \u0026#34;app.kubernetes.io/instance\u0026#34; = \u0026#34;coder-pvc-${lower(data.coder_workspace.me.owner)}-${lower(data.coder_workspace.me.name)}\u0026#34; \u0026#34;app.kubernetes.io/part-of\u0026#34; = \u0026#34;coder\u0026#34; // Coder specific labels. \u0026#34;com.coder.resource\u0026#34; = \u0026#34;true\u0026#34; \u0026#34;com.coder.workspace.id\u0026#34; = data.coder_workspace.me.id \u0026#34;com.coder.workspace.name\u0026#34; = data.coder_workspace.me.name \u0026#34;com.coder.user.id\u0026#34; = data.coder_workspace.me.owner_id \u0026#34;com.coder.user.username\u0026#34; = data.coder_workspace.me.owner } annotations = { \u0026#34;com.coder.user.email\u0026#34; = data.coder_workspace.me.owner_email } } wait_until_bound = false spec { access_modes = [\u0026#34;ReadWriteOnce\u0026#34;] resources { requests = { storage = \u0026#34;4Gi\u0026#34; } } } } resource \u0026#34;kubernetes_persistent_volume_claim\u0026#34; \u0026#34;docker\u0026#34; { metadata { name = \u0026#34;coder-${lower(data.coder_workspace.me.owner)}-${lower(data.coder_workspace.me.name)}-docker\u0026#34; namespace = var.namespace labels = { \u0026#34;app.kubernetes.io/name\u0026#34; = \u0026#34;coder-pvc\u0026#34; \u0026#34;app.kubernetes.io/instance\u0026#34; = \u0026#34;coder-pvc-${lower(data.coder_workspace.me.owner)}-${lower(data.coder_workspace.me.name)}\u0026#34; \u0026#34;app.kubernetes.io/part-of\u0026#34; = \u0026#34;coder\u0026#34; // Coder specific labels. \u0026#34;com.coder.resource\u0026#34; = \u0026#34;true\u0026#34; \u0026#34;com.coder.workspace.id\u0026#34; = data.coder_workspace.me.id \u0026#34;com.coder.workspace.name\u0026#34; = data.coder_workspace.me.name \u0026#34;com.coder.user.id\u0026#34; = data.coder_workspace.me.owner_id \u0026#34;com.coder.user.username\u0026#34; = data.coder_workspace.me.owner } annotations = { \u0026#34;com.coder.user.email\u0026#34; = data.coder_workspace.me.owner_email } } wait_until_bound = false spec { access_modes = [\u0026#34;ReadWriteOnce\u0026#34;] resources { requests = { storage = \u0026#34;10Gi\u0026#34; } } } } resource \u0026#34;kubernetes_persistent_volume_claim\u0026#34; \u0026#34;linuxbrew\u0026#34; { metadata { name = \u0026#34;coder-${lower(data.coder_workspace.me.owner)}-${lower(data.coder_workspace.me.name)}-linuxbrew\u0026#34; namespace = var.namespace labels = { \u0026#34;app.kubernetes.io/name\u0026#34; = \u0026#34;coder-pvc\u0026#34; \u0026#34;app.kubernetes.io/instance\u0026#34; = \u0026#34;coder-pvc-${lower(data.coder_workspace.me.owner)}-${lower(data.coder_workspace.me.name)}\u0026#34; \u0026#34;app.kubernetes.io/part-of\u0026#34; = \u0026#34;coder\u0026#34; // Coder specific labels. \u0026#34;com.coder.resource\u0026#34; = \u0026#34;true\u0026#34; \u0026#34;com.coder.workspace.id\u0026#34; = data.coder_workspace.me.id \u0026#34;com.coder.workspace.name\u0026#34; = data.coder_workspace.me.name \u0026#34;com.coder.user.id\u0026#34; = data.coder_workspace.me.owner_id \u0026#34;com.coder.user.username\u0026#34; = data.coder_workspace.me.owner } annotations = { \u0026#34;com.coder.user.email\u0026#34; = data.coder_workspace.me.owner_email } } wait_until_bound = false spec { access_modes = [\u0026#34;ReadWriteOnce\u0026#34;] resources { requests = { storage = \u0026#34;10Gi\u0026#34; } } } } resource \u0026#34;kubernetes_pod\u0026#34; \u0026#34;main\u0026#34; { count = data.coder_workspace.me.start_count metadata { name = \u0026#34;coder-${lower(data.coder_workspace.me.owner)}-${lower(data.coder_workspace.me.name)}\u0026#34; namespace = var.namespace labels = { \u0026#34;app.kubernetes.io/name\u0026#34; = \u0026#34;coder-workspace\u0026#34; \u0026#34;app.kubernetes.io/instance\u0026#34; = \u0026#34;coder-workspace-${lower(data.coder_workspace.me.owner)}-${lower(data.coder_workspace.me.name)}\u0026#34; \u0026#34;app.kubernetes.io/part-of\u0026#34; = \u0026#34;coder\u0026#34; // Coder specific labels. \u0026#34;com.coder.resource\u0026#34; = \u0026#34;true\u0026#34; \u0026#34;com.coder.workspace.id\u0026#34; = data.coder_workspace.me.id \u0026#34;com.coder.workspace.name\u0026#34; = data.coder_workspace.me.name \u0026#34;com.coder.user.id\u0026#34; = data.coder_workspace.me.owner_id \u0026#34;com.coder.user.username\u0026#34; = data.coder_workspace.me.owner } annotations = { \u0026#34;com.coder.user.email\u0026#34; = data.coder_workspace.me.owner_email } } spec { container { name = \u0026#34;code-container\u0026#34; image = \u0026#34;${data.coder_parameter.image.value}\u0026#34; image_pull_policy = \u0026#34;Always\u0026#34; command = [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, replace(coder_agent.main.init_script,\u0026#34;https://coder.org\u0026#34;,\u0026#34;http://coder\u0026#34;)] env { name = \u0026#34;CODER_AGENT_TOKEN\u0026#34; value = coder_agent.main.token } security_context { privileged = \u0026#34;true\u0026#34; } resources { requests = { \u0026#34;cpu\u0026#34; = \u0026#34;250m\u0026#34; \u0026#34;memory\u0026#34; = \u0026#34;512Mi\u0026#34; } limits = { \u0026#34;cpu\u0026#34; = \u0026#34;${data.coder_parameter.cpu.value}\u0026#34; \u0026#34;memory\u0026#34; = \u0026#34;${data.coder_parameter.memory.value}Gi\u0026#34; } } volume_mount { mount_path = \u0026#34;/root\u0026#34; name = \u0026#34;home\u0026#34; read_only = false } volume_mount { mount_path = \u0026#34;/var/lib/docker\u0026#34; name = \u0026#34;docker\u0026#34; read_only = false } volume_mount { mount_path = \u0026#34;/home/linuxbrew\u0026#34; name = \u0026#34;linuxbrew\u0026#34; read_only = false } } volume { name = \u0026#34;home\u0026#34; persistent_volume_claim { claim_name = kubernetes_persistent_volume_claim.home.metadata.0.name read_only = false } } volume { name = \u0026#34;docker\u0026#34; persistent_volume_claim { claim_name = kubernetes_persistent_volume_claim.docker.metadata.0.name read_only = false } } volume { name = \u0026#34;linuxbrew\u0026#34; persistent_volume_claim { claim_name = kubernetes_persistent_volume_claim.linuxbrew.metadata.0.name read_only = false } } } } Note: One quirk of this setup is that due to our environment using self signed certificate, we have to replace the external URL (the one used to access the envs) by the internal one (the coder service inside the ns) for our envs to start properly. In a more realistic scenario, trusted CA certs would be used instead.\nTo deploy this provider to our Coder instance, we\u0026rsquo;ll need the coder cli, available here. Depending on the exact setup (mainly due to self signed certificates), the login endpoint will vary, but the easiest is to port-forward the internal coder service and login through this:\n# in a tab kubectl port-forward svc/coder 8080:80 # in another tab, then follow the login procedure coder login http://127.0.0.1:8080 # once logged in, in the same dir as `main.tf`, reply yes to questions coder template create kube # You should preview the resources that will be created for each env: ┌──────────────────────────────────────────────────┐ │ Template Preview │ ├──────────────────────────────────────────────────┤ │ RESOURCE │ ├──────────────────────────────────────────────────┤ │ kubernetes_persistent_volume_claim.docker │ ├──────────────────────────────────────────────────┤ │ kubernetes_persistent_volume_claim.home │ ├──────────────────────────────────────────────────┤ │ kubernetes_persistent_volume_claim.linuxbrew │ ├──────────────────────────────────────────────────┤ │ kubernetes_pod.main │ │ └─ main (linux, amd64) │ └──────────────────────────────────────────────────┘ Accessing our environment Now everything required to work directly into our cluster is deployed. We can now create a Workspace based on the provider we defined earlier: Wait for the pod to be created and the code-server button to become available. Now we can work using a web browser from our thin \u0026amp; light laptop (or even a Raspberry Pi) with the power of a cluster:\nThere should have been a video here but your browser does not seem to support it. Disclaimer This setup is loosely based on what is deployed and used daily at $job. The main upsides are that :\nonboarding/updating/offboarding the environments are dead-easy, laptops are running way cooler, batteries last longer and builds are way faster, and testing has very little friction due to the fact that we are right inside the cluster. But there is some downsides:\nnotably around security, our envs are running as privileged in the cluster (for dockerd), which might not be fine depending on the level of trust you want. One possible fix would be to switch to a rootless docker/podman, but as of time of writing, there are too many quirks when deploying such solutions. having the dev envs centralised means possibly a SPOF that could ruin everyone\u0026rsquo;s day if something goes wrong with the platform or the cluster. it might require a certain level of maturity around containers, mainly around how networking is affected by a remote setup. ","permalink":"/posts/remote-env-with-coder/","summary":"\u003cp\u003e\u003cem\u003eA fleet of remote development environments (with docker, fish shell, and even minikube) running in your kubernetes cluster, accessible through VS Code in the browser !\u003c/em\u003e\u003c/p\u003e\n\u003cvideo class=\"video-shortcode\" preload=\"auto\" controls autoplay width=\"100%\"\u003e\n  \u003csource src=\"/coder-remote-env/demo.mp4\" type=\"video/mp4\"\u003e\n  There should have been a video here but your browser does not seem to support it.\n\u003c/video\u003e\n\u003ch1 id=\"what--why\"\u003eWhat \u0026amp; why\u003c/h1\u003e\n\u003cp\u003eThis setup is the v2 of a previous post on remote dev env \u003ca href=\"/posts/remote-mulituser-vscode-kubernetes/\"\u003eusing jupyterlab\u003c/a\u003e I made a year and a half ago. Thee OG setup was functionnal, but it had some issues, mainly around user management, container lifecycle and persistent data handling. As $dayjob has grown, so has the infrastructure, and so has the development needs. So a new solution was required.\u003c/p\u003e","title":"Remote development in Kubernetes With Coder"},{"content":"what \u0026amp; why In late 2022 while preparing a workshop around k8s for some french people, I realized that a lot of the k8s documentation isn\u0026rsquo;t translated in french.\nWhat to translate and what not to Albeit I\u0026rsquo;m not a fan of trying to translate the concepts name, ressources or objects revolving around k8s (for example PersistentVolumeClaim should not be translated as it\u0026rsquo;s used in config files, command lines args etc), it can be beneficial to translate the documentation itself for non-english native to better grasp a given concept.\nI then decided to step up my OSS contribution for 2023, by translating and improving the overall k8s documentation, focusing mainly on the tasks, which I think are crucial when learning a new concept.\nAnother goal of this project is to improve my comprehension of various k8s concepts, level-up my translation skills, and help out on a project that I\u0026rsquo;ve been using daily for the past 3 years.\nhow Current goal \u0026amp; progress My curent progress can be tracked through Gihub PRs\nAlthough I do not have a specific number of contributions in mind, I hope to translate around 2 or 3 tasks per month. We\u0026rsquo;ll see if that goal will age like milk or fine wine ;)\nStart contributing Contributing to the k8s documentation is fairly easy, and well documented:\nOnce you\u0026rsquo;ve forked the repo, create a new branch whose name will be defined by the type of contribution you\u0026rsquo;ll want to make (see the PR help for guidance.)\nThe project is using hugo as its building tool. You can either use it directly, or through a container to visualize your changes locally. In the repo dir, run\n# once to pull the submodules make module-init # then to build and serve the website make serve You can then access your changes on localhost:1313.\nFinally, commit your changes to your branch with an explicit message. Once you\u0026rsquo;ve good to go, open a PR. The first time, you\u0026rsquo;ll need to sign the CNCF CLA.\n","permalink":"/posts/translating-kubernetes/","summary":"\u003ch1 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h1\u003e\n\u003cp\u003eIn late 2022 while preparing a workshop around k8s for some french people,\nI realized that a lot of the \u003ca href=\"https://kubernetes.io/docs/home/\"\u003ek8s documentation\u003c/a\u003e isn\u0026rsquo;t translated in french.\u003c/p\u003e\n\u003ch2 id=\"what-to-translate-and-what-not-to\"\u003eWhat to translate and what not to\u003c/h2\u003e\n\u003cp\u003eAlbeit I\u0026rsquo;m not a fan of trying to translate the concepts name, ressources or objects revolving around k8s\n(for example \u003ccode\u003ePersistentVolumeClaim\u003c/code\u003e should not be translated as it\u0026rsquo;s used in config files, command lines args etc),\nit can be beneficial to translate the documentation itself for non-english native to better grasp a given concept.\u003c/p\u003e","title":"Contributing to the k8s documentation"},{"content":"what \u0026amp; why If you need to give access to your cluster to either another human or for a given service, you should create a dedicated account for it. This is how to do it.\nTo authenticate, humans can use both the ServiceAccount resource (through a token) and as Users (trough a key and crt). Bots or non-human things should only use ServiceAccounts.\nA word on RBAC Role Based Access Control (RBAC) is a way of separating users from privileges, by introducing roles. Instead of linking users to privlieges directly (Jake has read access on the pods), we link users to roles, which have a given set of privileges (Jake is a developper, and the developper role has read access on pods.). We can now attach multiple users to a role, and albeit it complexifies somewhat the number of ressources,\nIn Kubernetes, we need to create 3 resources when creating permissions:\na User (or ServiceAccount) a Role (bound to a given namespace) or clusterRole (spans through the cluster) that contains privileges, a RoleBinding or ClusterRoleBinding, that will bind our subject to the role. k8s doc on RBAC\nhow Users for humans Note: A part of the procedure should be done on a cluster\u0026rsquo;s node, as we need access to the control plane\u0026rsquo;s key \u0026amp; certificate.\nFirst, we\u0026rsquo;ll create a key for our user, here named jake:\nopenssl genrsa -out jake.key 2048 Now, we\u0026rsquo;ll create a CSR (Certificate Signing Request) that our cluster will sign:\nopenssl req \\ -new \\ -subj \u0026#34;/CN=jake\u0026#34; \\ -key jake.key \\ -out jake.csr This will create a .csr file, that we\u0026rsquo;ll sign using the certificate of the cluster:\nNote: some k8s distros do not store their pki file in /etc/kubernetes, check with their respective documentation on where they are.\nopenssl x509 -req \\ -in jake.csr \\ -CA /etc/kubernetes/pki/ca.crt \\ -CAkey /etc/kubernetes/pki/ca.key \\ -out jake.crt -days 365 This creates a jake.crt.\nNow we can use the .crt and .key that were created as an authentification method for our cluster. We\u0026rsquo;ll copy those to our machine. Let\u0026rsquo;s create a new user with the new auth method:\nkubectl config set-credentials jake --client-certificate=$PWD/jake.crt --client-key=$PWD/jake.key And now create a context with our new user\nkubectl config set-context jake-on-dev-cluster --cluster=dev-cluster --user=jake ServiceAccounts for non humans [Updated March 2022] If you are using a fairly recent (\u0026gt;= 1.22) version of kubectl which allows the creation of ressources through kubectl create, it\u0026rsquo;s now super easy to do so:\nCreate a new service account named bot1:\nkubectl create sa bot1 now, create a role (or cluster role) with the wanted permissions. Let\u0026rsquo;s say that the bot which will be using the service account will only need to get, list and watch pods and deployments in his namespace (default). (Further documentation can be retrieved using kubectl create role -h)\nkubectl create role bot1-pods --verb=get,list,watch --resource=pods,deploy Now, we\u0026rsquo;ll just bind the role (or cluster role) with our service account, using a rolebinding:\nthe service account must be specified using namespace:sa\nkubectl create rolebinding --serviceaccount default:bot1 --role bot1-pods bot1-pods nice ! Using only 3 commands we were able to create and configure our service account. Of course if further configuration on the role is needed (to have greater granularity), we can create a proper manifest or edit the ressource, but it gives us a good base to start with.\nNote for kubernetes \u0026gt;= 1.24 [Updated January 2023] Since kubernetes 1.24 (or 1.25 depending on the distro and flag activation), when creating a serviceaccount, kubernetes does not create an associated token containing a token. Release note about the change here.\nInstead, we should use Tokens:\nkubectl create token \u0026lt;serviceaccount_name\u0026gt; which will return a JWT token that can be used.\nTo retrieve the token linked to the service account, we can simply search the associated secret:\n# using jq kubectl get secrets -o json | jq \u0026#39;.items[] | select(.metadata.name|test(\u0026#34;bot1-token.*\u0026#34;)) | .data.token\u0026#39; # or using kubectl\u0026#39;s jsonpath kubectl get -o jsonpath=\u0026#34;{.data.token}\u0026#34; secret (kubectl get sa bot1 -o jsonpath=\u0026#34;{.secrets[0][\u0026#39;name\u0026#39;]}\u0026#34;) ServiceAccounts [OG July 2021] First, create a new serviceaccount, clusterrole (or role) \u0026amp; clusterrole binding (or role binding). Do not forget to change the permissions according to your needs :\napiVersion: v1 kind: ServiceAccount metadata: name: readonly namespace: \u0026lt;ns\u0026gt; --- # RB for the SA apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: readonly subjects: - kind: ServiceAccount name: readonly namespace: \u0026lt;ns\u0026gt; roleRef: kind: ClusterRole name: readonly apiGroup: rbac.authorization.k8s.io --- # Permissions for the SA apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: readonly rules: - apiGroups: [\u0026#39;\u0026#39;] # core API resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] apply it k apply -f user.yml\nNow grab the secret for that serviceaccount:\nk get secrets | grep readonly Tht will be something like readonly-token-wkp94. Then grab the secret\u0026rsquo;s token, and decode it from b64\nk get -o yaml secrets readonly-token-wkp94 | grep token: echo \u0026lt;token\u0026gt; | base64 -d You can now pass this token to whatever you need to.\nIf you need to setup a new human access (using kubectl for example) :\nkubectl config set-credentials newuser --token=\u0026#34;\u0026lt;token\u0026gt;\u0026#34; kubectl config set-context newuser-access --cluster=\u0026lt;clustername\u0026gt; --user=newuser kubectl config use-context newuser-access you should now be able to use kubectl, but with the permissions of that serviceAccount.\nImpersonating other users Using impersonation (which requires the impersonate verb on users, groups, and serviceaccounts in the core API group), we can test the newly created service account\u0026rsquo;s permissions:\ncan the account get pods ?\n\u0026gt; kubectl auth can-i --as \u0026#34;system:serviceaccount:namespace:serviceaccount_name\u0026#34; get pods yes can we get secrets ?\n\u0026gt; kubectl auth can-i --as \u0026#34;user\u0026#34; get secrets no What does this account has access to:\n\u0026gt; kubectl auth can-i --as \u0026#34;user\u0026#34; --list Resources Non-Resource URLs Resource Names Verbs deployments.apps [] [] [create get list update patch] statefulsets.apps [] [] [create get list update patch] ","permalink":"/posts/kube-user/","summary":"\u003ch1 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h1\u003e\n\u003cp\u003eIf you need to give access to your cluster to either another human or for a given service, you should create a dedicated account for it. This is how to do it.\u003c/p\u003e\n\u003cp\u003eTo authenticate, humans can use both the \u003ccode\u003eServiceAccount\u003c/code\u003e resource (through a token) and as \u003ccode\u003eUsers\u003c/code\u003e (trough a key and crt). Bots or non-human things should only use \u003ccode\u003eServiceAccounts\u003c/code\u003e.\u003c/p\u003e\n\u003ch2 id=\"a-word-on-rbac\"\u003eA word on RBAC\u003c/h2\u003e\n\u003cp\u003eRole Based Access Control (RBAC) is a way of separating users from privileges, by introducing \u003ccode\u003eroles\u003c/code\u003e. Instead of linking users to privlieges directly (Jake has read access on the pods), we link users to roles, which have a given set of privileges (Jake is a developper, and the developper role has read access on pods.). We can now attach multiple users to a role, and albeit it complexifies somewhat the number of ressources,\u003c/p\u003e","title":"Add a new external user (or bot) in k8s"},{"content":"Example of metrics exported from Apple Health in Grafana: If you want to test the tool, check out the Github repo here\nwhat \u0026amp; why Having a health tracker such as an apple watch is great, but the default views in the Health app on the iPhone can be too simplistic. We can\u0026rsquo;t correlate between metrics, define a specific time range, etc..\nBut it\u0026rsquo;s possible to export all of the collected health data in an archive. The goal then is to parse this archive and import it in a more analysis friendly tool. I\u0026rsquo;m most familiar with the InfluxDB+Grafana stack, so that\u0026rsquo;s what I\u0026rsquo;ll be using, but the parsing tool should provide a groundwork for parsing the exported data, and could be adapted to import to other tools.\nhow Architecture The tool is a 3 components docker-compose stack:\na parsing container that will ingest our exported health data, influxDB for storing the data grafana to visualize Export format From support.apple.com:\nShare your health and fitness data in XML format You can export all of your health and fitness data from Health in XML format, which is a common format for sharing data between apps. Tap your profile picture or initials at the top right. If you don’t see your profile picture or initials, tap Summary or Browse at the bottom of the screen, then scroll to the top of the screen. Tap Export all health data, then choose a method for sharing your data. This will create a .zip file that can be shared from the iPhone.\nOnce you\u0026rsquo;ve copied/share the file to your computer, unzip it. You should have a export.xml file in there. This file contains all the health records recorded by the Health app.\nThe format is as follow: k\n\u0026lt;Record type=\u0026#34;HKQuantityTypeIdentifierHeartRate\u0026#34; sourceName=\u0026#34;apple-watch\u0026#34; sourceVersion=\u0026#34;8.7\u0026#34; device=\u0026#34;\u0026amp;lt;\u0026amp;lt;HKDevice: 0x2803d9810\u0026amp;gt;, name:Apple Watch, manufacturer:Apple Inc., model:Watch, hardware:Watch3,4, software:8.7\u0026amp;gt;\u0026#34; unit=\u0026#34;count/min\u0026#34; creationDate=\u0026#34;2022-07-26 17:28:52 +0200\u0026#34; startDate=\u0026#34;2022-07-26 17:22:58 +0200\u0026#34; endDate=\u0026#34;2022-07-26 17:22:58 +0200\u0026#34; value=\u0026#34;73\u0026#34;\u0026gt; \u0026lt;MetadataEntry key=\u0026#34;HKMetadataKeyHeartRateMotionContext\u0026#34; value=\u0026#34;0\u0026#34;/\u0026gt; \u0026lt;/Record\u0026gt; Each record contains at least a type, which is the category of the metric, timestamps, a value and a unit. We also have access to the tracker that recorded the metric.\nThe ingester iterates through all the records, parses the important fields, then yields a dict that can be used as a measurement for influx.\nTips on analyzing the data Some metrics can be displayed as is, but others might need tweaking in the influx request:\nadjusting the time interval to 1d. using sum() instead of mean() to aggregate the metrics for a given interval. ","permalink":"/posts/exporting-apple-health-data/","summary":"\u003cp\u003e\u003cem\u003eExample of metrics exported from Apple Health in Grafana:\u003c/em\u003e\n\u003cimg loading=\"lazy\" src=\"https://github.com/k0rventen/apple-health-grafana/raw/main/example.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIf you want to test the tool, check out the \u003ca href=\"https://github.com/k0rventen/apple-health-grafana\"\u003eGithub repo here\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h2\u003e\n\u003cp\u003eHaving a health tracker such as an apple watch is great, but the default views in the Health app on the iPhone can be too simplistic. We can\u0026rsquo;t correlate between metrics, define a specific time range, etc..\u003c/p\u003e\n\u003cp\u003eBut it\u0026rsquo;s possible to export all of the collected health data in an archive. The goal then is to parse this archive and import it in a more analysis friendly tool. I\u0026rsquo;m most familiar with the InfluxDB+Grafana stack, so that\u0026rsquo;s what I\u0026rsquo;ll be using, but the parsing tool should provide a groundwork for parsing the exported data, and could be adapted to import to other tools.\u003c/p\u003e","title":"Exporting Apple Health Data"},{"content":"Avoid bursting and brute forcing on your flask app, with RFC6585 compliance\nwhat \u0026amp; why Rate limiting is a powerful way to restrict the use of a given service by allowing a given rate of requests. Banning on the other hand can be used to block malicious attacks, mainly brute forcing on authentification routes.\nThe Flask framework does not provide these functionnalities natively (which is normal, it\u0026rsquo;s a WSGI app constructor) but we can create a module to perform these features through flask\u0026rsquo;s primitives.\nThe goal is to create a simple module that can:\nrate limit all or any given subset of routes exposed by flask, ban IPs based on their behavior, support running behind a reverse proxy (when the client IP is the proxy\u0026rsquo;s, and the real client IP is somewhere in the headers) how Enter Flask-gatekeeper. It answers all the needs depicted above, but beware that it has some notable tradeoffs, mainly the fact that it\u0026rsquo;s a stateless module.\nLet\u0026rsquo;s have a look on how to use the module. We\u0026rsquo;ll first initialize it alongside our flask app:\napp = Flask(__name__) gk = GateKeeper(app, # or use .init_app(app) later ip_header=\u0026#34;x-my-ip\u0026#34;, # optionnal header to use for the client IP (e.g if using a reverse proxy) ban_rule={\u0026#34;count\u0026#34;:3,\u0026#34;window\u0026#34;:10,\u0026#34;duration\u0026#34;:600}, # 3 reports in a 10s window will ban for 600s rate_limit_rules=[{\u0026#34;count\u0026#34;:20,\u0026#34;window\u0026#34;:1},{\u0026#34;count\u0026#34;:100,\u0026#34;window\u0026#34;:10}], # rate limiting will be applied if over 20 requests in 1s or 100 requests in 10s excluded_methods=[\u0026#34;HEAD\u0026#34;]) # do not add HEAD requests to the tally The GateKeeper constructor takes somes self explanatory arguments that will configure the main instance. If running behind a reverse proxy, we can supply the header that will contain the IP of the og client (X-Real-IP if its Nginx for example) All requests will be added to the tally per client, including HEAD or OPTIONS requests. We can ignore specific methods using the excluded_methods arg.\nThen when we define routes, they will by default be subject to the rate limiting applied by the GateKeeper we defined above.\n@app.route(\u0026#34;/ping\u0026#34;) # this route is rate limited by the global rule def ping(): return \u0026#34;ok\u0026#34;,200 If we do not want to apply any rate limiting on a givern route, we can decorate the route as such:\n@app.route(\u0026#34;/bypass\u0026#34;) @gk.bypass # do not apply anything on that route def bypass(): return \u0026#34;ok\u0026#34;,200 Some routes might need additional, stricter rate limiting. In this case, we can define new rate limiting rules to be added on top on the ones we defined above:\n@app.route(\u0026#34;/global_plus_specific\u0026#34;) @gk.specific(rate_limit_rules=[{\u0026#34;count\u0026#34;:1,\u0026#34;window\u0026#34;:2}]) # add another rate limit on top of the global one (to avoid bursting for example) def specific(): return \u0026#34;ok\u0026#34;,200 We might want specific rate limiting for specific routes, for example a broader rule:\n@app.route(\u0026#34;/standalone\u0026#34;) @gk.specific(rate_limit_rules=[{\u0026#34;count\u0026#34;:10,\u0026#34;window\u0026#34;:3600}],standalone=True) # rate limited only by this rule def standalone(): return \u0026#34;ok\u0026#34;,200 Finally, we can control when IPs are banned using the .report() method. A generic use case would be to report the IP if the authentification failed, and it will be banned whenever the number of tries surpasses our rule.\n@app.route(\u0026#34;/login\u0026#34;) def login(): if request.json.get(\u0026#34;password\u0026#34;) == \u0026#34;password\u0026#34;: return token,200 else: gk.report() # report the request\u0026#39;s IP, after 3 reports in this case the IP will be banned return \u0026#34;bad password\u0026#34;,401 Let\u0026rsquo;s launch our app and try a few endpoints to see how it works. Note that the shell being used is fish, and some outputs are truncated for readability.\nfor i in (seq 11) http get :5000/standalone end [...] HTTP/1.1 429 TOO MANY REQUESTS Connection: close Content-Length: 72 Content-Type: text/html; charset=utf-8 Date: Mon, 27 Jun 2022 18:56:38 GMT Retry-After: 3441 Server: Werkzeug/2.1.2 Python/3.10.4 ip 127.0.0.1 rate limited for 3441s (over 10 requests in a 3600s window) When the rate limiting applies, as per the RFC6585, a HTTP code 429 is returned, with the Retry-After header containing in seconds the time to wait, and a short explanation present in the body.\nNow let\u0026rsquo;s try the banning:\nfor i in (seq 4) http get :5000/login password=notthegoodpwd end HTTP/1.1 401 UNAUTHORIZED Connection: close Content-Length: 12 Content-Type: text/html; charset=utf-8 Date: Mon, 27 Jun 2022 19:02:31 GMT Server: Werkzeug/2.1.2 Python/3.10.4 bad password [...] HTTP/1.1 403 FORBIDDEN Connection: close Content-Length: 63 Content-Type: text/html; charset=utf-8 Date: Mon, 27 Jun 2022 19:02:34 GMT Retry-After: 596 Server: Werkzeug/2.1.2 Python/3.10.4 ip 127.0.0.1 banned for 596s (reported 3 times in a 10s window) After 3 failed attempt, the default 401 reply is short-circuited by GateKeeper and a 403 is sent instead. As for the rate limiting, a short explanation is sent through the body, alongside a Retry-After header.\nTo give it a try or check the documentation, the module is available on PyPi, the code here on Github.\n","permalink":"/posts/flask-gatekeeper/","summary":"\u003cp\u003e\u003cem\u003eAvoid bursting and brute forcing on your flask app, with RFC6585 compliance\u003c/em\u003e\u003c/p\u003e\n\u003ch1 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h1\u003e\n\u003cp\u003eRate limiting is a powerful way to restrict the use of a given service by allowing a given rate of requests.\nBanning on the other hand can be used to block malicious attacks, mainly brute forcing on authentification routes.\u003c/p\u003e\n\u003cp\u003eThe Flask framework does not provide these functionnalities natively (which is normal, it\u0026rsquo;s a WSGI app constructor) but we can create a module to perform these features through flask\u0026rsquo;s primitives.\u003c/p\u003e","title":"Gatekeeper, a ban \u0026 rate limit lib for flask"},{"content":"Scale workloads based on the size of a rabbitMQ queue automatically, and have on-demand processing for any tasks\nThere should have been a video here but your browser does not seem to support it. A sped up example of autoscaling using KEDA with a rabbitMQ setup\nwhat \u0026amp; why Kubernetes is a great fit for autoscaling, and it already has a built-in system for doing autoscaling based on the metrics-server infos, like CPU usage for a pod. It\u0026rsquo;s quite easy to do that using the Horizontal Pod Autoscaler (HPA), and I made a demo system with it (here)[/posts/kube-hpa].\nBut some workloads can\u0026rsquo;t scale based on a CPU usage metrics for example, and we need another metric that better describe the load being applied to the system, and how it should respond to that load.\nEnter KEDA, the Kubernetes Event Driven Autoscaler. The goal of KEDA is to manage sources of metrics that can be used for autoscaling, and apply the corresponding scaling of resources.\nhow Let\u0026rsquo;s say I have producers that emits messages in the queue, each being a payload to process. On the other side of the queue are workers, which can process said payloads.\nThis could represent a client-server model where a client would request something to be processed, and our workers would handle each request. A real-world example could be a PDF report generation service, where users can request reports to be generated from arbitrary data. Or an platform ingesting various types of incoming data (say payloads from IOT devices) where incoming traffic is very variable.\nIn our specific use case, the producers and workers will be simple python container, with a rabbitMQ message broker in between.\nKeda provides a lot of integrations with various sources, message queues, cloud provider tools.. The list of their integrations can be found on their site.\nWe\u0026rsquo;ll focus on the rabbitMQ integration for now, but the general flow is the same regardless of the integration:\nWe instruct KEDA on the workload that should be scaled, and the settings aroung it (like the min and max number of replicas to have depending on the load), KEDA connects to a source of metrics that represents the load of the system, Depending on the current and past metric, KEDA tells kube to scale up or down the workload. Here is a schema of how KEDA operates:\nsetup To deploy our system , we\u0026rsquo;ll use the same setup as the previous post on scaling:\n# namespace k create ns app # rabbitmq message bus k create -n app deploy rabbitmq --image rabbitmq:3-management --port 5672 --port 15672 # expose rabbitMQ so we can check the UI and KEDA can check its status k expose -n app deploy/rabbitmq --port 5672,15672 # producer k create -n app deploy producer --image=k0rventen/hpa-server # worker k create -n app deploy worker --image=k0rventen/hpa-worker k0rventen/hpa-server and k0rventen/hpa-worker are the containers that act as producers and consumers based on a foo rabbitmq queue.\nIf we check the rabbitMQ queue, we can see the number of message is pilling up, because our single worker can\u0026rsquo;t handle the number of messages emitted by the producer. To do that, we can setup a port-forward between us and the rabbitmq interface:\nkubectl port-forward svc/rabbitmq 15672 -n app and then go to http://127.0.0.1:15672. default creds are guest:guest.\nInstallation Now, let\u0026rsquo;s install KEDA, following their documentation:\nhelm repo add kedacore https://kedacore.github.io/charts helm repo update helm install keda kedacore/keda --create-namespace --namespace keda You should now have a few pods running in the keda ns.\n\u0026gt; kubectl get pods -n keda NAME READY STATUS RESTARTS AGE keda-operator-7879dcd589-65t4x 1/1 Running 0 10m keda-operator-metrics-apiserver-54746f8fdc-fs4kb 1/1 Running 1 11m Using scalers Then, we\u0026rsquo;ll need to connect KEDA to our rabbitMQ queue. KEDA works based on scalers that connects to the source of metrics that should be used for scaling, We\u0026rsquo;ll also tell KEDA which deployment is our target for autoscaling:\napiVersion: v1 kind: Secret metadata: name: keda-rabbitmq-secret data: host: \u0026#34;aHR0cDovL2d1ZXN0Omd1ZXN0QHJhYmJpdG1xLmFwcDoxNTY3Mi8v\u0026#34; --- apiVersion: keda.sh/v1alpha1 kind: TriggerAuthentication metadata: name: keda-trigger-auth-rabbitmq-conn namespace: app spec: secretTargetRef: - parameter: host name: keda-rabbitmq-secret key: host --- apiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: worker-autoscaler namespace: app spec: scaleTargetRef: name: worker minReplicaCount: 0 maxReplicaCount: 20 pollingInterval: 2 cooldownPeriod: 20 advanced: horizontalPodAutoscalerConfig: behavior: scaleDown: stabilizationWindowSeconds: 20 policies: - type: Percent value: 50 periodSeconds: 20 triggers: - type: rabbitmq metadata: protocol: http queueName: foo mode: QueueLength value: \u0026#34;20\u0026#34; authenticationRef: name: keda-trigger-auth-rabbitmq-conn The file contains the following ressources:\nA secret which contains the full URL of our rabbitmq instance. Decoding it gives http://guest:guest@rabbitmq.app:15672//.\nThis is the URL that KEDA will use to connect to RabbitMQ. Note that we specify the namespace of the rabbitmq service, because KEDA will try to connect to rabbit from its own pod in the keda namespace. The last / in the URL is the name of the rabitmq vhost, which by default is /.\na TriggerAuthentication CRD that references the secret above, and binds it to a host key.\na ScaledObject CRD that defines our autoscaler:\nthe ressource to scale using scaleTargetRef, which is our worker deployment, various config settings regarding the scaling (min/max number of replicas, polling rate \u0026amp; cooldown after a ramp up), a horizontalPodAutoscalerConfig object that defines the behavior of the HPA: the policy here defines that half the pods can be stopped every period (20s) in a cooldown phase (when the load lightens). triggers that are used to scale the ressource: in our case: scaling should occur based on the length of the rabbitmq queue foo, and scale workers every 20 messages in the queue and to connect and authenticate to rabbitmq, KEDA shoud use the secret we created. We can check on the state of the autoscaler by checking on the ScaledObject ressource :\n\u0026gt; k get scaledobject -n app NAME SCALETARGETKIND SCALETARGETNAME MIN MAX TRIGGERS AUTHENTICATION READY ACTIVE FALLBACK AGE worker-autoscaler apps/v1.Deployment worker 1 20 rabbitmq keda-trigger-auth-rabbitmq-conn True True False 4m12s If everything went alright, the autoscaler should increase the number of replicas of our worker, to match what is defined in our autoscaling rule.\nWe can influence the number of workers created by adjusting the number of producers:\n\u0026gt; k scale deploy -n app producer --replicas \u0026lt;int\u0026gt; And we can watch the number of workers at the same time:\n\u0026gt; kubectl get pods -n app -w NAME READY STATUS RESTARTS AGE producer-5d9cb496cc-dvk2r 1/1 Running 0 28s producer-5d9cb496cc-dwx77 1/1 Running 0 28s rabbitmq-58ff5479cf-6657s 1/1 Running 0 67s worker-58b8d8c67f-btv4q 1/1 Running 0 7s worker-58b8d8c67f-v75lb 1/1 Running 0 17s worker-58b8d8c67f-6hbss 0/1 Pending 0 0s worker-58b8d8c67f-6hbss 0/1 Pending 0 0s worker-58b8d8c67f-6hbss 0/1 ContainerCreating 0 0s Another great advantage of using a message queue and this approach is that if no messages are in the queue, there is no need to have any worker at idle.\nIf we lower the minimum number of workers to 0, and if the queue does not have any messages for a given period of time, KEDA will simply scale to 0 the workers. And if a message is pushed to the queue, this will be trigger KEDA to scale up the workers to handle the request. That\u0026rsquo;s pretty handy in situations where we want \u0026lsquo;on-demand\u0026rsquo; scaling based on the current load.\nWe can tweak the various parameters to improve the responsiveness of the scaling using\nthe KEDA docs the HPA docs Screencast The screencast above is a 10 minutes run of the tutorial shown here, 2000x speed. The rabbitMQ interface shows the number of messages in the queue, while the terminal shows the pods being created/terminated by KEDA depending on the load.\n","permalink":"/posts/keda-autoscaling/","summary":"\u003cp\u003e\u003cem\u003eScale workloads based on the size of a rabbitMQ queue automatically, and have on-demand processing for any tasks\u003c/em\u003e\u003c/p\u003e\n\u003cvideo class=\"video-shortcode\" preload=\"auto\" controls autoplay width=\"100%\"\u003e\n  \u003csource src=\"/keda/keda_run.mp4\" type=\"video/mp4\"\u003e\n  There should have been a video here but your browser does not seem to support it.\n\u003c/video\u003e\n\u003cp\u003e\u003cem\u003e\u003ca href=\"#screencast\"\u003eA sped up example of autoscaling using KEDA with a rabbitMQ setup\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h2\u003e\n\u003cp\u003eKubernetes is a great fit for autoscaling, and it already has a built-in system for doing autoscaling based on the metrics-server infos, like CPU usage for a pod.\nIt\u0026rsquo;s quite easy to do that using the Horizontal Pod Autoscaler (HPA), and I made a demo system with it (here)[/posts/kube-hpa].\u003c/p\u003e","title":"Autoscaling using KEDA"},{"content":"what \u0026amp; why The CKS (Certified Kubernetes Security Specialist) is a great resource for knowing how to secure a kubernetes cluster. It covers a lot of topics, from the cluster side (admission controller, webhooks, audit), app side (Pod Security Policies) and supply chain (image scanning). Another great resource for this is the Kubernetes Hardening Guidance by NSA \u0026amp; CISA\nBut some of the concepts defined in both these resources are very case-specific, and require a lot of time, tools \u0026amp; effort to setup. In some environnements, it might be infeasible to deploy each and every one of those concepts. But that doesn\u0026rsquo;t mean we should avoid some basic security-minded steps when deploying to k8s. I won\u0026rsquo;t cover things on the cluster-side (audit, tools like falco, or admission controllers), but how you can improve the security of your front-facing app by adding a few lines here and there.\nhow Let say we have a python app, that exposes an API. We have a basic Dockerfile for it, and a simple deploy.yaml spec file containing our Deployment. They are what you could call \u0026rsquo;typical\u0026rsquo; of what can be found online when looking for a template of dockerfile or deployment:\nDockerfile:\nFROM python:3-alpine COPY requirements.txt . RUN pip3 install -r requirements.txt WORKDIR /app COPY src/ ./ ENV PYTHONUNBUFFERED 1 CMD python3 app.py deployment manifest:\napiVersion: apps/v1 kind: Deployment metadata: name: api spec: selector: matchLabels: app: api template: metadata: labels: app: api spec: containers: - name: api image: registry/api:latest ports: - containerPort: 5000 Let\u0026rsquo;s secure things up :\nnon root user The first recommendation is to run our containers as non-root users. For that, we\u0026rsquo;ll first add a few lines to our Dockerfile:\n... RUN addgroup -S app \u0026amp;\u0026amp; adduser -H -D -S app -G app -s /dev/null USER app:app WORKDIR /app Note that we are using an alpine-based container, so the exact command might vary on other distros, but the goal is the same\nBy creating a user \u0026lsquo;app\u0026rsquo;, and using it to run our app, we avoid giving way too much permissions to the app. We can check that by exec\u0026rsquo;ing into the running container:\n(before)\n/app # id uid=0(root) gid=0(root) groups=1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video) /app # ps PID USER TIME COMMAND 1 root 0:00 {flask-api.py} /usr/local/bin/python3 ./flask-api.py 7 root 0:00 sh 14 root 0:00 ps (now)\n/app $ id uid=101(app) gid=101(app) /app $ ps PID USER TIME COMMAND 1 app 0:00 {flask-api.py} /usr/local/bin/python3 ./flask-api.py 7 app 0:00 sh 14 app 0:00 ps Right, so now we are not running as root on the container side, but what about the host ? To which user are we mapped to on the host side ?\nwhen running the default image:\nk8s-worker \u0026gt; ps -aux | grep flask root 2266529 1.8 0.1 28168 23752 ? Ss 17:31 0:00 /usr/local/bin/python3 ./flask-api.py We are actually mapped to the root user ! that\u0026rsquo;s not the most secure setup ! If somehow an attacker gain control of the pod and is able to escape, he will land on the host as the root user.\nAnd if we use the new Dockerfile with the USER directive:\nps -aux | grep flask syslog 2267104 2.8 0.1 28168 23812 ? Ss 17:32 0:00 /usr/local/bin/python3 ./flask-api.py What ? Why are we mapped to the syslog user ? A quick check of /etc/passwd shows us why :\ncat /etc/passwd | grep 101 syslog:x:101:107::/home/syslog:/usr/sbin/nologin This is because when we created the user app in the Dockerfile, it assigned to it the uid 101, which is the same as the syslog user on our host.\nTo avoid clashing with a potential user with the same uid on the host, we can use a higher uid, in the 40000-60000 range.\nrunAsUser To fix that, we will tweak our deploy.yaml:\ncontainers: - name: api image: registry/api:latest securityContext: runAsUser: 60096 runAsGroup: 60096 Now, from the host side, we will appear as uid 60096, which isn\u0026rsquo;t mapped to a predefined user (unless a user with the same uid exists on the host obviously).\nreadOnlyRootFilesystem Another great addition to the Deployment spec, it to set the filesystem as readonly. This will block any attempt to modify the filesystem of the container, like installing binaries, modifying configuration in /etc..\ncontainers: - name: api image: registry/api:latest securityContext: readOnlyRootFilesystem: true runAsUser: 60096 runAsGroup: 60096 Now if we try to change something, we\u0026rsquo;ll be greeted by an error message, preventing any change on the root fs:\nERROR: Unable to lock database: Read-only file system ERROR: Failed to open apk database: Read-only file system If our app needs to be able to write in a given folder (like a /tmp/cache folder for caching), we can create a volumeMount in the deployment, mapped to an \u0026rsquo;emptyDir\u0026rsquo;. This will allow the app to write at this folder, while retaining the read-only root fs:\ncontainers: - name: api image: registry/api:latest volumeMounts: - mountPath: /tmp/cache name: cache-volume volumes: - name: cache-volume emptyDir: {} automountServiceAccountToken If the pod is not going to communicate with the kubernetes API, we can avoid mounting the service account\u0026rsquo;s token in the pod (which by default will be mounted in /var/run/secrets/kubernetes.io/serviceaccount/token):\nspec: automountServiceAccountToken: false containers: resources While we\u0026rsquo;re on the deployment, it\u0026rsquo;s also a good idea to set some resources limits on the pod. This will prevent the pods from consuming all the resources from the host, which even if it\u0026rsquo;s from a genuine mistake, can result in outages or other disurptions:\ncontainers: - name: api image: registry/api:latest resources: limits: cpu: 500m memory: 128Mi requests: cpu: 100m memory: 64Mi requests will tell k8s the requirements for the pod, i.e what we can expect the pod to consume. This will aid the scheduling on an appropriate node.\nlimits will actually stop the pod from consuming more than what is specified, either by throttling for the cpu, or OOM for the memory.\nSHA tagging Another recommendation would be to use SHA tagging on the base image of our Dockerfile. This serves two purposes:\nmaking reproducible builds possible. Otherwise, as the base image can be updated, this will break our current setup even though our Dockerfile hasn\u0026rsquo;t changed. aleviate supply chain attacks: If the base image used is subject to a supply chain attack (a threat actor injects/modify the image), our image becomes subject as well because we\u0026rsquo;ll pull the latest version of the image. To do so, we simply add the SHA of the image we want to fix at the end of the tag:\nFROM python:3.9-alpine3.15@sha256:f2aeefbeb3846b146a8ad9b995af469d249272af804b309318e2c72c9ca035b0 We can repeat the same behavior on the deployment. Instead of using registry/api:latest, but by using a versionning scheme (like semver or calver) and combining this with the SHA of the image, we ensure that the image used in the deployment will not be updated due to upstream changes.\nResults Final versions of the Dockerfile and deploy.yaml would look like this:\nDockerfile\nFROM python:3.9-alpine3.15@sha256:f2aeefbeb3846b146a8ad9b995af469d249272af804b309318e2c72c9ca035b0 COPY requirements.txt . RUN pip3 install -r requirements.txt RUN addgroup -S app \u0026amp;\u0026amp; adduser -H -D -S app -G app -s /dev/null USER app:app WORKDIR /app COPY src/ ./ ENV PYTHONUNBUFFERED 1 CMD [\u0026#34;python3\u0026#34;,\u0026#34;app.py\u0026#34;] deploy.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: api spec: selector: matchLabels: app: api template: metadata: labels: app: api spec: automountServiceAccountToken: false containers: - name: api image: registry/api:v1.0.3@sha256:f469d249272af80... securityContext: readOnlyRootFilesystem: true runAsUser: 69096 runAsGroup: 69096 ports: - containerPort: 5000 volumeMounts: - mountPath: /tmp/cache name: cache-volume volumes: - name: cache-volume emptyDir: {} Theses are simples yet effectives ways to secure your app on k8s. If you want to go deeper, I would suggests checking more advanced topics like host-side security (using apparmor profiles for example), falco , or supply chain security (Open Policy Agent, Admission controllers..). Theses are all topics that are covered by the CKS, and with plenty of information online.\nFurther resources https://pwning.systems/posts/escaping-containers-for-fun/ https://kubernetes.io/docs/concepts/storage/volumes/#emptydir https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ ","permalink":"/posts/basic-k8s-security/","summary":"\u003ch2 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h2\u003e\n\u003cp\u003eThe CKS (Certified Kubernetes Security Specialist) is a great resource for knowing how to secure a kubernetes cluster.\nIt covers a lot of topics, from the cluster side (admission controller, webhooks, audit), app side (Pod Security Policies) and supply chain (image scanning). Another great resource for this is the \u003ca href=\"https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETESHARDENINGGUIDANCE.PDF\"\u003eKubernetes Hardening Guidance by NSA \u0026amp; CISA\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eBut some of the concepts defined in both these resources are very case-specific, and require a lot of time, tools \u0026amp; effort to setup. In some environnements, it might be infeasible to deploy each and every one of those concepts. But that doesn\u0026rsquo;t mean we should avoid some basic security-minded steps when deploying to k8s. I won\u0026rsquo;t cover things on the cluster-side (audit, tools like falco, or admission controllers), but how you can improve the security of your front-facing app by adding a few lines here and there.\u003c/p\u003e","title":"A basic, security-minded k8s app setup"},{"content":"a fully featured, multi-user, dev env running on your k8s cluster that can coldboot in less than 10s\nThere should have been a video here but your browser does not seem to support it. what \u0026amp; why I use VS Code as my primary IDE for everything, from python development, to kubernetes/helm chart generation, even when building stuff for raspberry pico or arduino.\nMy daily workflow at work is :\ndevelop on vscode, mostly python/go, have docker installed and build my images on my computer, push them on a local (LAN) k8s cluster with a registry, redeploy on the cluster using skaffold by targeting the registry. But having most of the work done on my computer is not ideal. I need to have a shitload of stuff installed (vscode, dependencies, docker (which on mac runs on a vm).. ), and I have a massive cluster doing absolutely nothing out of that flow. So the goal here is to leverage as much as I possibly can from the server, and have the most lightweight local environment possible.\nAnd because this will be used to work as a team, we need this to be multi-user.\nBasically we want to go from that:\nto that :\nWith all that said, let\u0026rsquo;s deploy a multi user vscode environment in our kubernetes cluster !\nhow There is 2 main components that we\u0026rsquo;ll need to make this:\na docker image that will be ran in our cluster, and that will include everything we need a way of using this image as our remote environment (I assume an already running kubernetes cluster, if not there is a link to a guide in the second section)\nstep 1: making a code-server docker image The guys at CoderHQ have made a version of vscode that can run remotely (https://github.com/coder/code-server), and the interface is available through HTTP.\nSo we\u0026rsquo;ll use that to have VS Code running remotely. But we might want other things to be installed too, like a specific language env, a different shell\u0026hellip;\nHere is the Dockerfile I use, that installs some utils for kubernetes, python, code-server and the fish shell:\n# base FROM ubuntu:20.04 # tools, shell, language support RUN apt update RUN DEBIAN_FRONTEND=noninteractive apt install curl htop git dnsutils less fish nano openssh-client python3 python3-pip python3-venv -y --no-install-recommends # k8s tools (docker client, kubectl, k9s, skaffold) COPY --from=docker:dind /usr/local/bin/docker /usr/local/bin/docker RUN curl -Lo kubectl \u0026#34;https://dl.k8s.io/release/v1.22.0/bin/linux/amd64/kubectl\u0026#34; \u0026amp;\u0026amp; install kubectl /usr/local/bin/kubectl RUN curl -Lo skaffold \u0026#34;https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64\u0026#34; \u0026amp;\u0026amp; install skaffold /usr/local/bin/skaffold RUN curl -Lo k9s.tar.gz \u0026#34;https://github.com/derailed/k9s/releases/download/v0.25.8/k9s_Linux_x86_64.tar.gz\u0026#34; \u0026amp;\u0026amp; tar xvf k9s.tar.gz k9s \u0026amp;\u0026amp; install k9s /usr/local/bin/k9s # code-server RUN curl -fsSL https://code-server.dev/install.sh | sh # config \u0026amp; cleanup (set shell, copy config, apt cleanup.. ) RUN chsh -s /usr/bin/fish COPY config/ /tmp/code RUN chmod a+x /tmp/code/bootstrap.fish RUN rm kubectl skaffold k9s.tar.gz k9s RUN apt-get clean \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* /var/tmp/* # mounted work dir WORKDIR /root/projects CMD [\u0026#34;/tmp/code/bootstrap.fish\u0026#34;] the /tmp/code/bootstrap.fish mentionned as the CMD is a script that creates some dirs, moves config in the right places, then starts vscode:\n#! /usr/bin/fish # copies the default config to the home dir is not present # and launches code-server as main process if test ! -e /root/.code echo \u0026#34;creating ssh keys\u0026#34; mkdir /root/.ssh chmod 700 /root/.ssh ssh-keygen -b 2048 -N \u0026#34;\u0026#34; -f /root/.ssh/id_rsa echo \u0026#34;creating python venv\u0026#34; python3 -m venv /root/dev-env echo \u0026#34;creating config directories\u0026#34; mkdir -p /root/.config/code-server/data mkdir -p /root/.config/code-server/extensions mkdir -p /root/.config/fish echo \u0026#34;copying config\u0026#34; mv -v /tmp/code/config.fish /root/.config/fish/config.fish mv -v /tmp/code/config.yaml /root/.config/code-server/config.yaml touch /root/.code end echo \u0026#34;starting code server\u0026#34; exec code-server --config=/root/.config/code-server/config.yaml /root/projects We need to do this when the container starts instead of when generating the image, because the /root folder will be mounted as a volume in our container, making our work data and project persistant, even if we upgrade the image or restart the container.\nAlso, the config.yaml from the exec line at the end is the code-server config to use. It binds to port 8080, sets the config paths, disables auth, telemetry \u0026amp; update checs.\nbind-addr: 0.0.0.0:8080 auth: none user-data-dir: \u0026#34;/root/.config/code-server/data\u0026#34; extensions-dir: \u0026#34;/root/.config/code-server/extensions\u0026#34; cert: false disable-telemetry: true disable-update-check: true If we were to start this image locally using docker with -p 8080:8080, we would find a vscode running on that port.\nstep 2: managing users through jupyterhub Using a single code-server is not a viable option because we would appear a a single user and each user would overlap with each other, and storage would be a mess. Each user would need its own folder, and not to mess with others, etc.. Not practical.\nSo to manage that, we\u0026rsquo;ll use jupyterhub ! From their github repo (https://github.com/jupyterhub/jupyterhub):\nWith JupyterHub you can create a multi-user Hub that spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server.\nProject Jupyter created JupyterHub to support many users. The Hub can offer notebook servers to a class of students, a corporate data science workgroup, a scientific research project, or a high-performance computing group.\nAnd some smart folks have put an amazing tutorial here !\nSo for everything related to the basic setup, you can just follow along with their tutorial. It boils down to installing kube and installing helm.\nWe will also install docker as we only installed the client in our container, and will mount the docker socket in our pod. Note that this is not the most secure approach, and might get revised in the future.\nIt is also favorable to install an ingress with kubernetes, as that will allow us to use a proper DNS name with HTTPS to access our vscode.\nThe next step from the guide is to actually install jupyterhub. For that, we\u0026rsquo;ll use this values.yaml:\nsingleuser: image: name: \u0026#34;registry.gitlab.com/alpha-caeli/tooling/code-server\u0026#34; # where your image is stored tag: \u0026#34;latest\u0026#34; storage: homeMountPath: \u0026#34;/root\u0026#34; # for storage persistence proxy: chp: extraCommandLineFlags: - \u0026#34;--no-include-prefix\u0026#34; secretToken: seed_a_random_token_here # change that hub: extraConfig: code_spawner.py: | # use the kubespawner connector # https://github.com/jupyterhub/kubespawner from kubespawner.spawner import KubeSpawner c.JupyterHub.spawner_class = KubeSpawner code_settings.py: | # run our bootstrap script c.KubeSpawner.cmd = [] # always pull our image c.KubeSpawner.image_pull_policy = \u0026#34;Always\u0026#34; # as our user c.KubeSpawner.uid = None c.KubeSpawner.gid = None c.KubeSpawner.port = 8080 c.KubeSpawner.mem_guarantee = \u0026#34;400M\u0026#34; # use our own SA c.KubeSpawner.service_account = \u0026#34;code-server\u0026#34; c.KubeSpawner.automount_service_account_token = True # use user/pass to auth to jupyterhub c.JupyterHub.authenticator_class = \u0026#39;firstuseauthenticator.FirstUseAuthenticator\u0026#39; c.Authenticator.admin_users = {\u0026#39;admin\u0026#39;} # and mount the docker socket c.KubeSpawner.volume_mounts += [{\u0026#34;mountPath\u0026#34;:\u0026#34;/var/run/docker.sock\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;docker-sock\u0026#34;},] c.KubeSpawner.volumes += [{\u0026#34;name\u0026#34;:\u0026#34;docker-sock\u0026#34;,\u0026#34;hostPath\u0026#34;:{\u0026#34;path\u0026#34;:\u0026#34;/var/run/docker.sock\u0026#34;}}] (don\u0026rsquo;t forget to edit values.yaml with a random token (using openssl rand -hex 32 for example) and correct admin user)\nJupyterHub will use the KubeSpawner instance to talk to kubernetes and create on-the-fly a pod whenever a new user connects. Pretty cool.\nThen add the jupyterhub helm chart:\nhelm repo add jupyterhub https://jupyterhub.github.io/helm-chart/ helm repo update now deploy it :\nhelm upgrade --cleanup-on-fail \\ --install vs-code-remote jupyterhub/jupyterhub \\ --namespace \u0026lt;namespace\u0026gt; \\ --version=1.2.0 \\ --values values.yaml Wait a bit, then looking for pods with the jupyterhub label should output something like that:\n\u0026gt; k get pods -l app=jupyterhub NAME READY STATUS RESTARTS AGE proxy-757bbc65-6t2hv 1/1 Running 3 26d continuous-image-puller-tn6n4 1/1 Running 3 26d user-scheduler-7fcb988779-z4zr5 1/1 Running 1 4d8h user-scheduler-7fcb988779-sh2kj 1/1 Running 0 3d8h hub-6d94767744-xqb6s 1/1 Running 0 2d9h To access vscode securely, let\u0026rsquo;s create a ingress.yaml that routes the jupyterhub proxy svc to a given dns endpoint. Edit the host to match your cluster\u0026rsquo;s DNS endpoint (or none for the IP):\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: vs-code-remote spec: rules: - host: code.mycluster.lan http: paths: - backend: serviceName: proxy-public servicePort: 80 path: / pathType: Prefix tls: - hosts: - code.mycluster.lan --- apiVersion: v1 kind: Service metadata: annotations: meta.helm.sh/release-name: vs-code-remote labels: app: jupyterhub app.kubernetes.io/managed-by: Helm chart: jupyterhub-1.2.0 component: proxy-public heritage: Helm release: vs-code-remote name: proxy-public spec: ports: - name: http port: 80 protocol: TCP targetPort: http selector: component: proxy release: vs-code-remote sessionAffinity: None type: ClusterIP status: loadBalancer: {} We\u0026rsquo;ll also create a rbac.yaml that creates a dedicated service account for the vs-code pod, so that we can talk to the kube api from there (to deploy/restart services).\napiVersion: v1 automountServiceAccountToken: true kind: ServiceAccount metadata: name: code-server --- # RB for the SA apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: code-server subjects: - kind: ServiceAccount name: code-server roleRef: kind: Role name: code-server apiGroup: rbac.authorization.k8s.io --- # Permissions for the SA apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: code-server rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;,\u0026#34;pods/log\u0026#34;,\u0026#34;services\u0026#34;,\u0026#34;persistentvolumeclaims\u0026#34;,\u0026#34;configmaps\u0026#34;,\u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;create\u0026#34;,\u0026#34;patch\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;delete\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] resources: [\u0026#34;deployments\u0026#34;,\u0026#34;statefulsets\u0026#34;,\u0026#34;replicasets\u0026#34;,\u0026#34;daemonsets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;create\u0026#34;,\u0026#34;patch\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;delete\u0026#34;] - apiGroups: [\u0026#34;networking.k8s.io\u0026#34;] resources: [\u0026#34;ingresses\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;create\u0026#34;,\u0026#34;patch\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;delete\u0026#34;] We can apply those now:\n# we have to delete the service beforehand because we can\u0026#39;t patch when changing the type of Service k delete svc proxy-public # apply our mods k apply -f ingress.yaml k apply -f rbac.yaml You can now go to the endpoint, and you\u0026rsquo;ll be greeted by the jupyterhub login page.\nThe first time you login, it will record your user/password combo.\nOnce you\u0026rsquo;re in, a pod will be created for your environment, and you should see the familiar vscode interface.\nThere should have been a video here but your browser does not seem to support it. And voila! You can now use the power of your cluster to handle the majority of your workflow, while making your laptop cooler and maybe quieter !\n","permalink":"/posts/remote-mulituser-vscode-kubernetes/","summary":"\u003cp\u003e\u003cem\u003ea fully featured, multi-user, dev env running on your k8s cluster that can coldboot in less than 10s\u003c/em\u003e\u003c/p\u003e\n\u003cvideo class=\"video-shortcode\" preload=\"auto\" controls autoplay width=\"100%\"\u003e\n  \u003csource src=\"/remote-vscode/demo.mp4\" type=\"video/mp4\"\u003e\n  There should have been a video here but your browser does not seem to support it.\n\u003c/video\u003e\n\u003ch1 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h1\u003e\n\u003cp\u003eI use VS Code as my primary IDE for everything, from python development, to kubernetes/helm chart generation, even when building stuff for raspberry pico or arduino.\u003c/p\u003e\n\u003cp\u003eMy daily workflow at work is :\u003c/p\u003e","title":"Remote, multi-user VSCode running in kubernetes"},{"content":"what \u0026amp; why During a project, I needed to build a container that could render graphs based on pretty big arrays, using plotly, kaleido and pandas.\nThe arrays would be DataFrames from pandas, turned into graphs through plotly, and then renderer as jpeg images using kaleido.\nThis is not uncommon to have pretty big dependencies in a python project, but when pulling these pacakges locally, it took quite a long time, so I checked the size of each :\nroot@69ee6367d91f:/usr/local/lib/python3.9/site-packages# du -sh . 536M . root@69ee6367d91f:/usr/local/lib/python3.9/site-packages# du -sh * | sort -h | tail -n 5 30M numpy 33M numpy.libs 58M pandas 140M plotly 221M kaleido Well, a 536M dependencies folder. Turns out that kaleido embeds the open-source Chromium browser as a library, and plotly and pandas are both pretty big dependencies by themselves.\nThat won\u0026rsquo;t make for a nice and small container. Let\u0026rsquo;s see how we can improve things !\nhow First, let\u0026rsquo;s try to create a basic, simple docker image with these dependencies.\nrequirements.txt\npandas==1.3.1 kaleido==0.2.1 plotly==5.2.1 flask==2.0.1 basic dockerfile A very simple dockerfile for this might look like this :\nFROM python:3.9 WORKDIR /build COPY requirements.txt . RUN pip3 install -r requirements.txt COPY app.py . # launch CMD [\u0026#34;python3\u0026#34;,\u0026#34;-u\u0026#34;,\u0026#34;app.py\u0026#34;] That\u0026rsquo;s fine, but the resulting image is quite heavy :\n\u0026gt; docker images | grep renderer renderer full 35b81d0fd3f7 10 minutes ago 1.5GB Oh well. A 1.5GB container. That\u0026rsquo;s Windows territory ! Surely we can go under a GB.\nmulti layer We can use the multi-layer system of docker to build a smaller image.\nHere we have two improvements :\nwe are using a slim image as our final layer, which is lighter than the full python:3.9. Note that due to our dependencies, we can\u0026rsquo;t use an alpine based image, otherwise we would have been much lower regarding the size. we only copy what we need from the build layer. The dockerfile might look like this :\n# build layer FROM python:3.9 as builder WORKDIR /build COPY requirements.txt . RUN pip3 install --prefix /install -r requirements.txt # final layer FROM python:3.9-slim WORKDIR /app COPY --from=builder /install /usr/local COPY app.py . # launch CMD [\u0026#34;python3\u0026#34;,\u0026#34;-u\u0026#34;,\u0026#34;app.py\u0026#34;] Once we\u0026rsquo;ve built the dependencies in the build layer, we copy the site-packages folder to the final layer.\n\u0026gt; docker images | grep renderer renderer slim 5401960f2a66 10 seconds ago 577MB renderer full 35b81d0fd3f7 16 minutes ago 1.5GB That\u0026rsquo;s better. Only 577MB, this is just shy of 1/3 the original size.\nhere comes the compression This is where something occured to me.\nWhy not compress the whole dependency folder when building, and decompress it on-the-go when starting the container ?\nHere is the Dockerfile:\n# build layer FROM python:3.9 as builder RUN apt update \u0026amp;\u0026amp; apt install xz-utils -y WORKDIR /build COPY requirements.txt . RUN pip3 install -r requirements.txt RUN XZ_OPT=\u0026#34;-T 0\u0026#34; tar Jcf packages.tar.xz /usr/local/lib/python3.9/site-packages/ # final layer FROM python:3.9-slim RUN apt update \u0026amp;\u0026amp; apt install xz-utils --no-install-recommends -y \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* WORKDIR /app COPY --from=builder /build/packages.tar.xz . COPY app.py . # launch CMD [\u0026#34;python3\u0026#34;,\u0026#34;-u\u0026#34;,\u0026#34;app.py\u0026#34;] In our app.py, before importing the libraries:\n# before loading modules, decompress the site-package archive import subprocess decompress = subprocess.run([\u0026#34;tar\u0026#34;,\u0026#34;Jxf\u0026#34;, \u0026#34;packages.tar.xz\u0026#34; ,\u0026#34;-C\u0026#34;, \u0026#34;/\u0026#34;],env={\u0026#34;XZ_OPT\u0026#34;:\u0026#34;-T 0\u0026#34;},capture_output=True) import ... Looking at the produced container image :\n\u0026gt; docker images | grep renderer renderer xz d1def5592c6e 18 seconds ago 208MB renderer slim 5401960f2a66 4 minutes ago 577MB renderer full 35b81d0fd3f7 16 minutes ago 1.5GB 208MB! That\u0026rsquo;s not bad.\nDocker compresses the image when uploaded to the registry, so let\u0026rsquo;s push our images and see what the size differences are there:\nThe gains are impressive when using a slim/multi-layer dockerfile. And by compressing our libs we gained 50MB, a ~30% improvement.\nconclusions This is fun, but not an ideal solution for handling \u0026ldquo;big\u0026rdquo; containers:\nSize-wise, the final container is still 208MB, which is a 63% decrease in size!\nbut once uploaded to a registry, and the image being compressed as a whole there, the size decrease by only 29%.\nWhen starting the container, you\u0026rsquo;ll need to decompress the packages, which (on my machine) took a few seconds. This should also be taken into account.\nIf network bandwidth was a constraint, this solution will not help much. You\u0026rsquo;ll pull 50MB less, but that\u0026rsquo;s not groundbreaking.\nThe only use case I can see is if you have storage limitations on the receiving end, as the savings are much more important once the whole image is decompressed by docker. But you will still need to decompress the package when launching a container with this image, so the gains are limited to the image.\nDespite this whole setup being not-as-practical as I thought it would initially (before reminding myself of the whole docker registry compression..), this could still be somewhat (maybe) useful in the future, so i\u0026rsquo;ll still post it..\n","permalink":"/posts/reducing-docker-images-size-using-xz/","summary":"\u003ch2 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h2\u003e\n\u003cp\u003eDuring a project, I needed to build a container that could render graphs based on pretty big arrays, using \u003ccode\u003eplotly\u003c/code\u003e, \u003ccode\u003ekaleido\u003c/code\u003e and \u003ccode\u003epandas\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe arrays would be DataFrames from pandas, turned into graphs through plotly, and then renderer as jpeg images using kaleido.\u003c/p\u003e\n\u003cp\u003eThis is not uncommon to have pretty big dependencies in a python project, but when pulling these pacakges locally, it took quite a long time, so I checked the size of each :\u003c/p\u003e","title":"Reducing Docker Images Size Using Xz"},{"content":"what \u0026amp; why From principlesofchaos.org :\nAdvances in large-scale, distributed software systems are changing the game for software engineering. As an industry, we are quick to adopt practices that increase flexibility of development and velocity of deployment. An urgent question follows on the heels of these benefits: How much confidence we can have in the complex systems that we put into production?\nApplying this philosophy to kube is a very pertinent thing to do, but how ? The same website defines Chaos Engineering as the discipline of experimenting on a system in order to build confidence in the system’s capability to withstand turbulent conditions in production.\nSo, let\u0026rsquo;s introduce turbulences in our cluster.\nNetflix developed their Chaos Monkey a while back, and this is basically the same thing, applied to kubernetes concepts.\nA monkey, in the cluster, breaking things. The easiest route for this is attacking the primary Kube resource, pods. Let\u0026rsquo;s terminate some random pods and see what happens.\nWe are sure that kubernetes will restart our pods if we have defined any kind of top level management over them (such as deployments, daemonsets, etc..). Killing a pod is about testing whether our application can withstand it.\nhow This is also my first real golang project, using some of the language\u0026rsquo;s specific features about concurrency (go routines, channels) and structs.\nThe project is made of 3 go routines, each communicating with the other using channels. The configuration is handled through a struct.\nThe main routine is a simple sleeps until the next cron occurence, notify the killer routine, repeat loop. The killer routine waits for a message from the main routine, then list pods that match the criterias, select one of them and terminate it, and sends a message to the slack routine about what happened. the slack routine, which waits for a message and simply transmits the message to the channel (if any). This might not be the simplest approach, as a simple sleep-\u0026gt;kill-\u0026gt;message-\u0026gt;loop would have sufficed, but this is also about learning the ins-and-outs of the language.\nthe project is hosted here, and can be deployed on any cluster in 3 commands:\n# download the spec file curl -LO https://raw.githubusercontent.com/k0rventen/macaque/main/macaque.yml # edit the env vars to match your config $EDITOR macaque.yml # apply (make sure you are in the right ns) kubectl apply -f macaque.yml The containers are available for both amd64 and arm64. The RBAC configuration is included in the spec file, the pod needs list and delete of the pods resource in the current namespace.\nThe configuration is done through env var. You must specify the crontab spec and the namespace in which to kill pods, and you can add a label selector to narrow the targets, add a slack token and channel ID to be informed when the monkey does things.\n","permalink":"/posts/a-monkey-in-the-cluster/","summary":"\u003ch2 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h2\u003e\n\u003cp\u003eFrom \u003ca href=\"https://principlesofchaos.org\"\u003eprinciplesofchaos.org\u003c/a\u003e :\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eAdvances in large-scale, distributed software systems are changing the game for software engineering. As an industry, we are quick to adopt practices that increase flexibility of development and velocity of deployment. An urgent question follows on the heels of these benefits: How much confidence we can have in the complex systems that we put into production?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eApplying this philosophy to kube is a very pertinent thing to do, but how ? The same website defines Chaos Engineering as \u003cem\u003ethe discipline of experimenting on a system in order to build confidence in the system’s capability to withstand turbulent conditions in production.\u003c/em\u003e\u003c/p\u003e","title":"A Monkey in the Cluster"},{"content":"what \u0026amp; why Let\u0026rsquo;s say you have a scalable architecture (like a server/worker model), and you want autoscaling to happens automatically based on the workers CPU usage, which is useful is some scenarios. Kubernetes has an Horizontal Pod Autoscaler feature that we can utilize to do just that !\nhow First, let\u0026rsquo;s talk requirements. You\u0026rsquo;ll need :\na k8s cluster (k0s, minikube or microk8s), kubectl installed and configured to talk to your cluster metrics-server deployed. This will provide the metrics necessary for the autoscaling algorithm to work. Check on your particular provider how to do so. example architecture Here is an example architecture that can benefit from scaling :\na server that sends out jobs X workers that do_work() when receiving a job a way for the server to communicate with the workers, a message queue for example. In our demo, we have the following resources running on a k8s cluster :\na rabbitmq deployment, with a rabbitmq service.\na server deployment, based on k0rventen/hpa-server, with the following logic:\nimport time, json from random import choices, randint from string import ascii_letters import pika QUEUE_NAME = \u0026#34;foo\u0026#34; connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#39;rabbitmq\u0026#39;)) channel = connection.channel() channel.queue_declare(queue=QUEUE_NAME) if __name__ == \u0026#34;__main__\u0026#34;: while True: obj = {\u0026#34;payload\u0026#34;:\u0026#34;\u0026#34;.join(choices(ascii_letters,k=10))} channel.basic_publish(exchange=\u0026#34;\u0026#34;, routing_key=QUEUE_NAME,body=json.dumps(obj)) time.sleep(randint(1,11)/10) It basically connects to the rabbitmq broker, declare a new foo queue, and then sends message to that queue forever, every .1 to 1s (which averages to around .5s).\na worker deployment, running k0rventen/hpa-worker, with this code :\nimport time, pika QUEUE_NAME = \u0026#34;foo\u0026#34; connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#39;rabbitmq\u0026#39;)) channel = connection.channel() def callback(ch, method, properties, body): t0 = time.time() while time.time() \u0026lt; t0+.8: # work for .8s, which is a bit more than the interval between jobs sent by the server 1*1 ch.basic_ack(delivery_tag = method.delivery_tag) if __name__ == \u0026#34;__main__\u0026#34;: channel.basic_consume(queue=QUEUE_NAME,on_message_callback=callback) channel.start_consuming() the worker connects to the queue declared by the server, and for each message, it works for .8s.\nThe worker takes around .8s to process the job. Based on how many jobs per second are sent by the server, we might run into a situation where the worker is overloaded, and can\u0026rsquo;t keep up. Jobs will just pile up, and that\u0026rsquo;s not good. But it\u0026rsquo;s also pointless to just scale the workers manually, eg k scale deploy worker --replicas 6, and have too many instances just waiting when no jobs are sent, but 6 might be not enough if we encounter a spike in usage.\nTo follow along, create the following resources :\n# namespace k create ns hpa # rabbitmq message bus k create -n hpa deploy rabbitmq --image rabbitmq:3-management --port 5672 --port 15672 k expose -n hpa deploy/rabbitmq --port 5672 --target-port 5672 # server k create -n hpa deploy server --image=k0rventen/hpa-server # worker k create -n hpa deploy worker --image=k0rventen/hpa-worker we can now connect to the rabbitmq ui using k port-forward -n hpa deploy/rabbitmq 15672:15672 and opening localhost:15672.\nHPA based on CPU usage For the autoscaling to work, we need to specify what cpu usage percent is considered a threshold to spawn new instances. With metric-server, we have the raw cpu usage for each pod (let say our worker consume 900m cpu). But that doesn\u0026rsquo;t translate into percentage until we specify limits for our containers. For that, we\u0026rsquo;ll edit the worker deployment, and add resource limits to the container spec:\nk edit deploy/worker -n hpa\n... spec: containers: - image: k0rventen/hpa-worker imagePullPolicy: Always name: hpa-worker resources: limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;100m\u0026#34; ... We are enforcing a limit of 100m cpu. The single worker will now be pinned at 100m CPU usage, which you can check via kz top pods -n hpa | grep worker.\nWe can now configure our HPA to scale the number of replicas of our worker deployment with the following settings :\nk autoscale -n hpa deploy worker --max 8 --min 1 --cpu-percent 50\n--min is the minimum number of replicas to have. --max is the maximum number of replicas to have. Note that this number may not be achieved due to resources constraints. A simple example is with a host having 2 CPUs (or 2000m CPU), and a worker consuming 200m CPU, you won\u0026rsquo;t be able to have more than 10 replicas. --cpu-percent is the percentage of CPU usage above which the autoscaler will add new instances (and vice-versa). We can check what the hpa is doing with kz describe hpa -n hpa worker :\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulRescale 4m18s horizontal-pod-autoscaler New size: 3; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 2m33s horizontal-pod-autoscaler New size: 4; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 33s horizontal-pod-autoscaler New size: 5; reason: cpu resource utilization (percentage of request) above target On the rabbitmq dashboard, we can see the number of messages queued going downhill since the activation of the HPA :\nAnd once the queued messages are all processed, the workers\u0026rsquo;s CPU usage will drop, and the HPA will decrease the number of replicas to match that level of load. In our case it might drop to 2 or 3 replicas :\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulRescale 24m horizontal-pod-autoscaler New size: 3; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 22m horizontal-pod-autoscaler New size: 4; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 20m horizontal-pod-autoscaler New size: 5; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 8m30s horizontal-pod-autoscaler New size: 4; reason: All metrics below target Normal SuccessfulRescale 3m27s horizontal-pod-autoscaler New size: 3; reason: All metrics below target ","permalink":"/posts/kube-hpa/","summary":"\u003ch2 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s say you have a scalable architecture (like a server/worker model), and you want autoscaling to happens automatically based on the workers CPU usage, which is useful is some scenarios. Kubernetes has an \u003ccode\u003eHorizontal Pod Autoscaler\u003c/code\u003e feature that we can utilize to do just that !\u003c/p\u003e\n\u003ch2 id=\"how\"\u003ehow\u003c/h2\u003e\n\u003cp\u003eFirst, let\u0026rsquo;s talk requirements. You\u0026rsquo;ll need :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea \u003ccode\u003ek8s cluster\u003c/code\u003e (k0s, minikube or microk8s),\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekubectl\u003c/code\u003e installed and configured to talk to your cluster\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emetrics-server\u003c/code\u003e deployed. This will provide the metrics necessary for the autoscaling algorithm to work. Check on your particular provider how to do so.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"example-architecture\"\u003eexample architecture\u003c/h2\u003e\n\u003cp\u003eHere is an example architecture that can benefit from scaling :\u003c/p\u003e","title":"Exploring Kube's Horizontal Pod Autoscaler"},{"content":"what \u0026amp; why If you want to control a docker instance (the docker daemon) which is not your machine, you can expose it as a TCP socket (instead of a traditionnal UNIX socket) and connect to it remotely using the docker client. We\u0026rsquo;ll also use SSH forwarding to secure the connection to the docker api if security is a concern.\nhow Install docker\ncurl -fsSL https://get.docker.com | sh sudo usermod -aG docker $USER then edit the systemd service unit by adding the -H tcp://0.0.0.0:2375 to the ExecStart options.\nsudo systemctl edit docker.service [Service] ExecStart= ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 --containerd=/run/containerd/containerd.sock reload \u0026amp; restart\nsudo systemctl daemon-reload sudo systemctl restart docker.service FYI, the first ExecStart= is to remove the corresponding statement from the docker.service unit file. If we omit this, systemd will complain like Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services. Refusing.\nWe can check on the server that docker is indeed listening on port 2375 by running sudo ss -tulnp | grep 2375:\ntcp LISTEN 0 4096 *:2375 *:* users:((\u0026#34;dockerd\u0026#34;,pid=11239,fd=7)) On our local machine we should now be able to do docker -H \u0026lt;server_ip:2375\u0026gt; ps.\nImportant note: using -p or -v will not forward ports/volumes on your local machine but on the server.\nssh tunnel Finally, if don\u0026rsquo;t want to expose the bare docker API to our network, we can wrap it in a SSH tunnel !\n(you can use docker -H ssh://example.com ps directly, but it\u0026rsquo;s good to know that you can do it using ssh tunneling like a unix greybeard.)\nI assume you have ssh enabled on the server, and copied your pubkeys to make the connection passwordless. If not, enable ssh then ssh-copy-id \u0026lt;remote_user\u0026gt;@\u0026lt;server_ip\u0026gt;.\nWe can also change the override we made earlier of the docker daemon, from -H tcp://0.0.0.0:2375 to -H tcp://127.0.0.1:2375, so we don\u0026rsquo;t expose the docker API on each interface but only the loopback. (don\u0026rsquo;t forget to reload\u0026amp;restart).\nOn our local machine, we\u0026rsquo;ll do ssh -L 8375:127.0.0.1:2375 -N \u0026lt;remote_user\u0026gt;@\u0026lt;server_ip\u0026gt;.\n-L means to use the SSH connection to port forward to our port 8375 the port 127.0.0.1:2375 from the server point of view, which is its docker tcp socket. -N means to not launch any command You can now do docker -H tcp://127.0.0.1:8375 ps from our local machine, we we are now connecting through our port 8375, which is forwarded by SSH to the port 2375 of the server.\nBut we we have a look at the ssh man page, we can read this for -L:\n-L [bind_address:]port:host:hostport -L [bind_address:]port:remote_socket -L local_socket:host:hostport -L local_socket:remote_socket I was intrigued by [bind_address:]port:remote_socket, specifically by remote_socket. That would mean we can also forward unix socket. We know that on the server, the docker API is also exposed on /var/run/docker.sock.\nlet\u0026rsquo;s try ssh -L 8375:/var/run/docker.sock -N \u0026lt;remote_user\u0026gt;@\u0026lt;server_ip\u0026gt;. We should redirect the docker unix socket on the server to our host\u0026rsquo;s port 8375.\n\u0026gt; docker -H tcp://127.0.0.1:8375 ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES sure enough, it works !\nAs a bonus point we could disable completely the use of a TCP connection now, as we are using the unix socket on the server.\n","permalink":"/posts/remote-docker/","summary":"\u003ch2 id=\"what--why\"\u003ewhat \u0026amp; why\u003c/h2\u003e\n\u003cp\u003eIf you want to control a docker instance (the docker daemon) which is not your machine, you can expose it as a TCP socket (instead of a traditionnal UNIX socket) and connect to it remotely using the docker client. We\u0026rsquo;ll also use SSH forwarding to secure the connection to the docker api if security is a concern.\u003c/p\u003e\n\u003ch2 id=\"how\"\u003ehow\u003c/h2\u003e\n\u003cp\u003eInstall docker\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ecurl -fsSL https://get.docker.com | sh\nsudo usermod -aG docker $USER\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ethen edit the systemd service unit by adding the \u003ccode\u003e-H tcp://0.0.0.0:2375\u003c/code\u003e to the \u003ccode\u003eExecStart\u003c/code\u003e options.\u003c/p\u003e","title":"Remote Docker"}]