<!doctype html><html lang=en><head><title>Exploring Kube's Horizontal Pod Autoscaler ::
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="what &amp;amp; why Let&amp;rsquo;s say you have a scalable architecture (like a server/worker model), and you want autoscaling to happens automatically based on the workers CPU usage, which is useful is some scenarios. Kubernetes has an Horizontal Pod Autoscaler feature that we can utilize to do just that !
how First, let&amp;rsquo;s talk requirements. You&amp;rsquo;ll need :
a k8s cluster (k0s, minikube or microk8s), kubectl installed and configured to talk to your cluster metrics-server deployed."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=/posts/kube-hpa/><link rel=stylesheet href=/assets/style.css><link rel=stylesheet href=/style.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/img/favicon.png><link href=/assets/fonts/Inter-Italic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/assets/fonts/Inter-Regular.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/assets/fonts/Inter-Medium.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/assets/fonts/Inter-MediumItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/assets/fonts/Inter-Bold.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/assets/fonts/Inter-BoldItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><meta name=twitter:card content="summary"><meta name=twitter:title content="Exploring Kube's Horizontal Pod Autoscaler"><meta name=twitter:description content="scale your microservices based on CPU usage"><meta property="og:title" content="Exploring Kube's Horizontal Pod Autoscaler"><meta property="og:description" content="scale your microservices based on CPU usage"><meta property="og:type" content="article"><meta property="og:url" content="/posts/kube-hpa/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-07-27T00:00:00+00:00"><meta property="article:modified_time" content="2021-07-27T00:00:00+00:00"></head><body class=dark-theme><div class=container><header class=header><span class=header__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg>
</span><span class=logo__text>k0rventen:~</span>
<span class=logo__cursor></span>
</a><span class=header__right><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/links>links</a></li><li><a href=/manpage>manpage</a></li><li><a href=/>posts</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/links>links</a></li><li><a href=/manpage>manpage</a></li><li><a href=/>posts</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg>
</span><span class=theme-toggle><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><div class=post><h1 class=post-title>Exploring Kube&rsquo;s Horizontal Pod Autoscaler</h1><div class=post-meta><span class=post-date>2021-07-27
</span><span class=post-read-time>— 5 min read</span></div><span class=post-tags><a href=/tags/k8s/>#k8s</a>&nbsp;
<a href=/tags/hpa/>#hpa</a>&nbsp;
<a href=/tags/scaling/>#scaling</a>&nbsp;</span><div class=post-content><h2>Table of Contents</h2><aside class=table-of-contents><nav id=TableOfContents><ul><li><ul><li><a href=#what--why>what & why</a></li><li><a href=#how>how</a></li><li><a href=#example-architecture>example architecture</a></li><li><a href=#hpa-based-on-cpu-usage>HPA based on CPU usage</a></li></ul></li></ul></nav></aside><h2 id=what--why>what & why</h2><p>Let&rsquo;s say you have a scalable architecture (like a server/worker model), and you want autoscaling to happens automatically based on the workers CPU usage, which is useful is some scenarios. Kubernetes has an <code>Horizontal Pod Autoscaler</code> feature that we can utilize to do just that !</p><h2 id=how>how</h2><p>First, let&rsquo;s talk requirements. You&rsquo;ll need :</p><ul><li>a <code>k8s cluster</code> (k0s, minikube or microk8s),</li><li><code>kubectl</code> installed and configured to talk to your cluster</li><li><code>metrics-server</code> deployed. This will provide the metrics necessary for the autoscaling algorithm to work. Check on your particular provider how to do so.</li></ul><h2 id=example-architecture>example architecture</h2><p>Here is an example architecture that can benefit from scaling :</p><ul><li>a server that sends out jobs</li><li>X workers that do_work() when receiving a job</li><li>a way for the server to communicate with the workers, a message queue for example.</li></ul><p>In our demo, we have the following resources running on a k8s cluster :</p><ul><li><p>a <code>rabbitmq</code> deployment, with a <code>rabbitmq</code> service.</p></li><li><p>a <code>server</code> deployment, based on <code>k0rventen/hpa-server</code>, with the following logic:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>import</span> time<span style=color:#f92672>,</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> random <span style=color:#f92672>import</span> choices, randint
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> string <span style=color:#f92672>import</span> ascii_letters
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pika
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>QUEUE_NAME <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;foo&#34;</span>
</span></span><span style=display:flex><span>connection <span style=color:#f92672>=</span> pika<span style=color:#f92672>.</span>BlockingConnection(pika<span style=color:#f92672>.</span>ConnectionParameters(<span style=color:#e6db74>&#39;rabbitmq&#39;</span>))
</span></span><span style=display:flex><span>channel <span style=color:#f92672>=</span> connection<span style=color:#f92672>.</span>channel()
</span></span><span style=display:flex><span>channel<span style=color:#f92672>.</span>queue_declare(queue<span style=color:#f92672>=</span>QUEUE_NAME)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        obj <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;payload&#34;</span>:<span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(choices(ascii_letters,k<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>))}
</span></span><span style=display:flex><span>        channel<span style=color:#f92672>.</span>basic_publish(exchange<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&#34;</span>, routing_key<span style=color:#f92672>=</span>QUEUE_NAME,body<span style=color:#f92672>=</span>json<span style=color:#f92672>.</span>dumps(obj))
</span></span><span style=display:flex><span>        time<span style=color:#f92672>.</span>sleep(randint(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>11</span>)<span style=color:#f92672>/</span><span style=color:#ae81ff>10</span>)
</span></span></code></pre></div><p>It basically connects to the rabbitmq broker, declare a new <code>foo</code> queue, and then sends message to that queue forever, every .1 to 1s (which averages to around .5s).</p></li><li><p>a <code>worker</code> deployment, running <code>k0rventen/hpa-worker</code>, with this code :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>import</span> time<span style=color:#f92672>,</span> pika
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>QUEUE_NAME <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;foo&#34;</span>
</span></span><span style=display:flex><span>connection <span style=color:#f92672>=</span> pika<span style=color:#f92672>.</span>BlockingConnection(pika<span style=color:#f92672>.</span>ConnectionParameters(<span style=color:#e6db74>&#39;rabbitmq&#39;</span>))
</span></span><span style=display:flex><span>channel <span style=color:#f92672>=</span> connection<span style=color:#f92672>.</span>channel()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>callback</span>(ch, method, properties, body):
</span></span><span style=display:flex><span>    t0 <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> time<span style=color:#f92672>.</span>time() <span style=color:#f92672>&lt;</span> t0<span style=color:#f92672>+</span><span style=color:#ae81ff>.8</span>: <span style=color:#75715e># work for .8s, which is a bit more than the interval between jobs sent by the server</span>
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1</span><span style=color:#f92672>*</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    ch<span style=color:#f92672>.</span>basic_ack(delivery_tag <span style=color:#f92672>=</span> method<span style=color:#f92672>.</span>delivery_tag)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    channel<span style=color:#f92672>.</span>basic_consume(queue<span style=color:#f92672>=</span>QUEUE_NAME,on_message_callback<span style=color:#f92672>=</span>callback)
</span></span><span style=display:flex><span>    channel<span style=color:#f92672>.</span>start_consuming()
</span></span></code></pre></div><p>the worker connects to the queue declared by the server, and for each message, it <em>works</em> for .8s.</p></li></ul><p>The worker takes around .8s to process the job. Based on how many jobs per second are sent by the server, we might run into a situation where the worker is overloaded, and can&rsquo;t keep up. Jobs will just pile up, and that&rsquo;s not good. But it&rsquo;s also pointless to just scale the workers manually, eg <code>k scale deploy worker --replicas 6</code>, and have <em>too many</em> instances just waiting when no jobs are sent, but 6 might be not enough if we encounter a spike in usage.</p><p>To follow along, create the following resources :</p><pre tabindex=0><code># namespace
k create ns hpa

# rabbitmq message bus
k create -n hpa deploy rabbitmq --image rabbitmq:3-management --port 5672 --port 15672
k expose -n hpa deploy/rabbitmq --port 5672 --target-port 5672

# server
k create -n hpa deploy server --image=k0rventen/hpa-server

# worker
k create -n hpa deploy worker --image=k0rventen/hpa-worker
</code></pre><p>we can now connect to the rabbitmq ui using <code>k port-forward -n hpa deploy/rabbitmq 15672:15672</code> and opening <code>localhost:15672</code>.</p><h2 id=hpa-based-on-cpu-usage>HPA based on CPU usage</h2><p>For the autoscaling to work, we need to specify what cpu usage percent is considered a threshold to spawn new instances. With metric-server, we have the raw cpu usage for each pod (let say our worker consume 900m cpu). But that doesn&rsquo;t translate into percentage until we specify limits for our containers. For that, we&rsquo;ll edit the worker deployment, and add resource limits to the container spec:</p><p><code>k edit deploy/worker -n hpa</code></p><pre tabindex=0><code>    ...
    spec:
      containers:
      - image: k0rventen/hpa-worker
        imagePullPolicy: Always
        name: hpa-worker
        resources:
          limits:
            memory: &#34;128Mi&#34;
            cpu: &#34;100m&#34;
      ...
</code></pre><p>We are enforcing a limit of 100m cpu. The single worker will now be pinned at 100m CPU usage, which you can check via <code>kz top pods -n hpa | grep worker</code>.</p><p>We can now configure our HPA to scale the number of replicas of our <code>worker</code> deployment with the following settings :</p><p><code>k autoscale -n hpa deploy worker --max 8 --min 1 --cpu-percent 50</code></p><ul><li><code>--min</code> is the minimum number of replicas to have.</li><li><code>--max</code> is the maximum number of replicas to have. Note that this number may not be achieved due to resources constraints. A simple example is with a host having 2 CPUs (or 2000m CPU), and a worker consuming 200m CPU, you won&rsquo;t be able to have more than 10 replicas.</li><li><code>--cpu-percent</code> is the percentage of CPU usage above which the autoscaler will add new instances (and vice-versa).</li></ul><p>We can check what the hpa is doing with <code>kz describe hpa -n hpa worker</code> :</p><pre tabindex=0><code>Events:
  Type    Reason             Age    From                       Message
  ----    ------             ----   ----                       -------
  Normal  SuccessfulRescale  4m18s  horizontal-pod-autoscaler  New size: 3; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  2m33s  horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  33s    horizontal-pod-autoscaler  New size: 5; reason: cpu resource utilization (percentage of request) above target
</code></pre><p>On the rabbitmq dashboard, we can see the number of messages queued going downhill since the activation of the HPA :</p><p><img src=/kube-hpa/rabbitmq.png alt></p><p>And once the queued messages are all processed, the workers&rsquo;s CPU usage will drop, and the HPA will decrease the number of replicas to match that level of load. In our case it might drop to 2 or 3 replicas :</p><pre tabindex=0><code>Events:
  Type    Reason             Age    From                       Message
  ----    ------             ----   ----                       -------
  Normal  SuccessfulRescale  24m    horizontal-pod-autoscaler  New size: 3; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  22m    horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  20m    horizontal-pod-autoscaler  New size: 5; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  8m30s  horizontal-pod-autoscaler  New size: 4; reason: All metrics below target
  Normal  SuccessfulRescale  3m27s  horizontal-pod-autoscaler  New size: 3; reason: All metrics below target
</code></pre></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=/posts/a-monkey-in-the-cluster/><span class=button__icon>←</span>
<span class=button__text>A Monkey in the Cluster</span>
</a></span><span class="button next"><a href=/posts/remote-docker/><span class=button__text>Remote Docker</span>
<span class=button__icon>→</span></a></span></div></div></div></div><footer class=footer><div class=footer__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg>
</span><span class=logo__text>k0rventen:~</span>
<span class=logo__cursor></span></a><div class=copyright><span>© 2024 Powered by
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a></span>
<span>Theme created by
<a href=https://twitter.com/panr target=_blank rel=noopener>panr</a></span></div></div></footer><script src=/assets/main.js></script><script src=/assets/prism.js></script></div></body></html>